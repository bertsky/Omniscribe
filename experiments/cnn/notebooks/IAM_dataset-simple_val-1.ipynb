{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 with IAM dataset\n",
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main imports complete\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "from utils import *\n",
    "from classes import *\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print('main imports complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1. define data transform\n",
    "#\n",
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "# ========================================\n",
    "\n",
    "# Need the __name__ check to make multiprocessing work on Windows for some reason\n",
    "# if __name__ == '__main__':\n",
    "print(\"Set up data transforms.\")\n",
    "img_input_size = 56\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        # RandomRotation does not seem to be working on Windows right now\n",
    "        transforms.RandomRotation(45),\n",
    "\n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "\n",
    "        transforms.Grayscale(), # not sure why the current input is not grayscale, do grayscale conversion\n",
    "        transforms.Resize((img_input_size,img_input_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # should not do random transformation in val or test set\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((img_input_size,img_input_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((img_input_size,img_input_size)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "net1_data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#         transforms.Resize(64),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "        \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "#         transforms.Resize(64),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    \n",
    "    'test': transforms.Compose([\n",
    "#         transforms.Resize(64),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# data_transforms = selected_transforms\n",
    "data_transforms = net1_data_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train count: 7723\n",
      "val count: 3478\n",
      "test count: 2115\n"
     ]
    }
   ],
   "source": [
    "all_data_dir = '/home/rahul/data/handwriting/sampled/dim64x64-stride8x8/train_val_test-3'\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "imagefolders = {t : datasets.ImageFolder(os.path.join(all_data_dir, t),\n",
    "                                        transform = data_transforms[t])\n",
    "               for t in set_types}\n",
    "for t in set_types:\n",
    "    print(str(t) + ' count:', len(imagefolders[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device_name cuda\n",
      "Creating Net1.\n",
      "\n",
      "Epoch 0/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.2806224985921011\n",
      "f1_pos: 0.8876270962630397\n",
      "f1_neg: 0.8919090562682587\n",
      "\n",
      "Phase:val\n",
      "average loss: 0.4754975786183057\n",
      "f1_pos: 0.7446577327055415\n",
      "f1_neg: 0.831942789034565\n",
      "\n",
      "\n",
      "Epoch 1/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.17536366190199373\n",
      "f1_pos: 0.9305207646671061\n",
      "f1_neg: 0.9329601831828012\n",
      "\n",
      "Phase:val\n",
      "average loss: 2.427759244429647\n",
      "f1_pos: 0.12714558169103624\n",
      "f1_neg: 0.7449377670443991\n",
      "\n",
      "\n",
      "Epoch 2/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.16618609210667054\n",
      "f1_pos: 0.9347306977978244\n",
      "f1_neg: 0.9377845220030349\n",
      "\n",
      "Phase:val\n",
      "average loss: 0.5575465607601997\n",
      "f1_pos: 0.6625615763546798\n",
      "f1_neg: 0.8181415929203539\n",
      "\n",
      "\n",
      "Epoch 3/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.15470679316182034\n",
      "f1_pos: 0.9393779652082235\n",
      "f1_neg: 0.9414609315347418\n",
      "\n",
      "Phase:val\n",
      "average loss: 1.220470105045619\n",
      "f1_pos: 0.6432491767288694\n",
      "f1_neg: 0.32319866722199087\n",
      "\n",
      "\n",
      "Epoch 4/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.1534155628351015\n",
      "f1_pos: 0.9427706880673595\n",
      "f1_neg: 0.9445506692160612\n",
      "\n",
      "Phase:val\n",
      "average loss: 1.4649821794053615\n",
      "f1_pos: 0.634485757773429\n",
      "f1_neg: 0.28680526092490455\n",
      "\n",
      "\n",
      "Epoch 5/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.14777131374695288\n",
      "f1_pos: 0.9412851299643752\n",
      "f1_neg: 0.9434346002288039\n",
      "\n",
      "Phase:val\n",
      "average loss: 1.516073121429518\n",
      "f1_pos: 0.2817711328349627\n",
      "f1_neg: 0.7605903776116543\n",
      "\n",
      "\n",
      "Epoch 6/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.14728350414098984\n",
      "f1_pos: 0.9436507936507936\n",
      "f1_neg: 0.9459802181080395\n",
      "\n",
      "Phase:val\n",
      "average loss: 1.458356631563065\n",
      "f1_pos: 0.23990498812351546\n",
      "f1_neg: 0.7572078907435509\n",
      "\n",
      "\n",
      "Epoch 7/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.13685309207186278\n",
      "f1_pos: 0.9436656969055804\n",
      "f1_neg: 0.9459665144596652\n",
      "\n",
      "Phase:val\n",
      "average loss: 3.987491720363009\n",
      "f1_pos: 0.017543859649122806\n",
      "f1_neg: 0.7340153452685422\n",
      "\n",
      "\n",
      "Epoch 8/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.13383974201584609\n",
      "f1_pos: 0.949295403661267\n",
      "f1_neg: 0.950974150006367\n",
      "\n",
      "Phase:val\n",
      "average loss: 2.0988061960021813\n",
      "f1_pos: 0.17385943279901356\n",
      "f1_neg: 0.7487814023247094\n",
      "\n",
      "\n",
      "Epoch 9/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.12552769844293996\n",
      "f1_pos: 0.9488531505404691\n",
      "f1_neg: 0.9506361323155216\n",
      "\n",
      "Phase:val\n",
      "average loss: 2.6589506466269426\n",
      "f1_pos: 0.06060606060606061\n",
      "f1_neg: 0.7377712394262597\n",
      "\n",
      "\n",
      "Epoch 10/999\n",
      "----------\n",
      "Phase:train\n",
      "average loss: 0.13875618538221485\n",
      "f1_pos: 0.9478134495970405\n",
      "f1_neg: 0.9498540053319793\n",
      "\n",
      "Phase:val\n",
      "average loss: 0.7823100823859773\n",
      "f1_pos: 0.6100840336134453\n",
      "f1_neg: 0.7972027972027972\n",
      "\n",
      "\n",
      "Epoch 11/999\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "from model_utils import create_model_architecture\n",
    "from train_utils import train\n",
    "\n",
    "use_gpu = True\n",
    "device_name = 'cuda' if use_gpu else 'cpu'\n",
    "print('device_name', device_name)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "phases = ['train', 'val']\n",
    "metric_types = ['loss', 'tn', 'fp', 'fn', 'tp', 'f1_pos', 'f1_neg']\n",
    "metric_types_plot = ['loss', 'f1_pos', 'f1_neg']\n",
    "metric_names_all = [p+'-'+m for p in phases for m in metric_types]\n",
    "metric_names_plot = [p+'-'+m for p in phases for m in metric_types_plot]\n",
    "\n",
    "all_metrics = {metric : [] for metric in metric_names_all}\n",
    "\n",
    "num_training_epochs = 1000\n",
    "\n",
    "data_loaders = {t : torch.utils.data.DataLoader(imagefolders[t], batch_size = batch_size, shuffle = True, num_workers = 4)\n",
    "                for t in phases}\n",
    "\n",
    "model = create_model_architecture('net1', use_gpu = use_gpu)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "trained_model, train_metrics = train(model,\n",
    "                                     criterion,\n",
    "                                     optimizer,\n",
    "                                     data_loaders['train'],\n",
    "                                     data_loaders['val'],\n",
    "                                     num_epochs=num_training_epochs,\n",
    "                                     use_gpu = use_gpu)\n",
    "\n",
    "all_metrics = train_metrics\n",
    "\n",
    "for metric in metric_types_plot:\n",
    "    plot_values(all_metrics['train-' + str(metric)],\n",
    "               all_metrics['val-' + str(metric)],\n",
    "               str(metric) + \" over \" + str(num_training_epochs) + \" epochs\",\n",
    "               ylabel = str(metric))\n",
    "\n",
    "# write metrics to file\n",
    "# write_metrics_to_csv(all_metrics, metric_names_all, 'net1-IAM-aa_64-raw_logs', 'all_metrics.csv')\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write metrics to file\n",
    "write_metrics_to_csv(all_metrics, metric_names_all, 'net1-aa_64-1000ep-raw_logs', 'all_metrics.csv')\n",
    "torch.save(trained_model.state_dict(), 'models/net1-aa_64-1000ep.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sys\n",
    "# import os\n",
    "# sys.path.append(os.path.abspath('../../../'))\n",
    "# from detection.lib.model.ImageROI import ImageROI\n",
    "# from PIL import Image\n",
    "\n",
    "# # book_data_dir = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images'\n",
    "# # img_loc = book_data_dir + '/Dryden/positive/307.png'\n",
    "# # Testing with image of dimensions 1000x1381\n",
    "\n",
    "# img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Allestree/positive/1307.png'\n",
    "# pilimg = Image.open(img_loc)\n",
    "# w, h = pilimg.size\n",
    "# imgrois = [ImageROI(0, 0, w, h)]\n",
    "# imgarr = np.asarray(pilimg)\n",
    "\n",
    "# from bounding_box_classification import get_pos_rois\n",
    "# # net1 = Net1()\n",
    "\n",
    "# # Modify this as necessary\n",
    "# # imgrois = [ImageROI(0, 0, 1000, 700), ImageROI(0, 750, 1000, 500)]\n",
    "# trained_model.train(False)\n",
    "# with torch.set_grad_enabled(False):\n",
    "#     pos_rois = get_pos_rois(trained_model,\n",
    "#                             (pilimg, imgrois),\n",
    "#                             model_transform = net1_data_transforms['test'],\n",
    "#                             model_input_size = (64,64),\n",
    "#                             stride=(32, 32))\n",
    "\n",
    "# for rois in pos_rois:\n",
    "#     for roi in rois:\n",
    "#         print(roi.toString())\n",
    "        \n",
    "# all_rois = [roi for rois in pos_rois for roi in rois]\n",
    "    \n",
    "# print('number of pos ROIs found', len(all_rois))\n",
    "\n",
    "# from detection.lib.ImgProcessor import ImgProcessor\n",
    "# from detection.lib.utils.Utils import plt_img\n",
    "# # plot the rois and plot only certain information according to the set\n",
    "# # imgProcessor = ImgProcessor()\n",
    "# # img = imgProcessor.loadImage(inFilename=img_loc)\n",
    "# roi_level_set = set([1])\n",
    "# plt_img(imgarr, rois=all_rois, roi_level_set=roi_level_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sys\n",
    "# import os.path\n",
    "# sys.path.append(os.path.abspath('../../../'))\n",
    "\n",
    "\n",
    "# # from detection.lib.ImgProcessor import ImgProcessor\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from detection.lib.utils.Utils import *\n",
    "# from detection.lib.OCREngine import OCREngine\n",
    "# import pickle\n",
    "\n",
    "# from PIL import Image\n",
    "\n",
    "# # 1. parameters\n",
    "# # root_dir = './sample'\n",
    "# # img_id = '639'\n",
    "# # fname = '%s.png' %img_id\n",
    "# # ocr_engine = 'cv2'\n",
    "# # f = '%s/%s' % (root_dir,fname)\n",
    "\n",
    "# ocr_engine = 'tesseract'\n",
    "# img_id = '1307'\n",
    "# fname = '1307.png'\n",
    "# f = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Allestree/positive/1307.png'\n",
    "\n",
    "# # 2. Load the image\n",
    "# # imgProcessor = ImgProcessor()\n",
    "# # img = imgProcessor.loadImage(inFilename=f)\n",
    "# img = np.asarray(Image.open(f))\n",
    "\n",
    "# # 3. preprocess the image\n",
    "# # img = imgProcessor.quickPreprocess(img)\n",
    "\n",
    "# # 4. load the image ground truth\n",
    "# # img_roi_ground_truth = all_ground_truth[img_id]\n",
    "\n",
    "# # 5. define the OCR\n",
    "# # define engine\n",
    "# ocr = OCREngine(ocr_engine)\n",
    "# img_roi_ocr, data_df = ocr.image_to_data(img)\n",
    "\n",
    "# # plot the rois and plot only certain information according to the set\n",
    "# # roi_level_set = set([2])\n",
    "# # rois = [(img_roi_ground_truth,'b'), (img_roi_ocr,'r')]\n",
    "# rois = [(img_roi_ocr,'r')]\n",
    "# plt_img(img, rois=rois)#, roi_level_set=roi_level_set)\n",
    "\n",
    "# # save the plot\n",
    "# plt.savefig('./result/%s_%s_boxes.png' %(fname,ocr_engine))\n",
    "\n",
    "# # save the bounding boxes to txt\n",
    "# # save_rois(img_roi_ground_truth,'./result/%s_imgROIs_ground_truth.txt' %fname)\n",
    "# save_rois(img_roi_ocr,'./result/%s_imgROIs.txt' %fname)\n",
    "\n",
    "# # save the bounding boxes to pickle\n",
    "# # with open('./result/%s_imgROIs.pkl' %fname,'wb') as f:\n",
    "# #     pickle.dump({'ocr':img_roi_ocr, 'ground_truth':img_roi_ground_truth},f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
