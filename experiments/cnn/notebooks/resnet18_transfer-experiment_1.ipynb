{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 with resnet18 transfer learning.\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment 1 with resnet18 transfer learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports 1 complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "import torchnet as tnt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"imports 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a class to deal with early stopping criteria\n",
    "# Important: minimizes a loss value\n",
    "class EarlyStopping:\n",
    "    def __init__(self, min_delta=0, patience=5):\n",
    "        # The minimum delta in loss to be considered a change in loss\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        # number of epochs to wait for improvement before terminating\n",
    "        self.patience = patience\n",
    "        \n",
    "        # number of epochs waited\n",
    "        self.wait = 0\n",
    "        \n",
    "        # Set \"best loss\" to some large number\n",
    "        self.best_loss = 1e15\n",
    "        \n",
    "    def checkStoppingCriteria(self, curr_loss):\n",
    "        \"\"\" Returns whether the stopping criteria has been met. \"\"\"\n",
    "        if (curr_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = curr_loss\n",
    "            self.wait = 1\n",
    "        elif self.wait < self.patience:\n",
    "            self.wait += 1\n",
    "        else:\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 'train_model' function is a generic routine that can be used to train various models.\n"
     ]
    }
   ],
   "source": [
    "print(\"This 'train_model' function is a generic routine that can be used to train various models.\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, data_loaders, num_epochs=25, early_stopping = None):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    val_acc_loss = 0.0\n",
    "    \n",
    "#     epoch_val_accs = {}\n",
    "#     epoch_train_accs = {}\n",
    "    epoch_acc_dict = {\"train\": {}, \"val\" : {}}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Confusion matrix\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "            \n",
    "            count = 0\n",
    "            \n",
    "            # Iterate over each book\n",
    "            for book in data_loaders[phase]:\n",
    "                \n",
    "                # Iterate over data.\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                    count += len(inputs)\n",
    "                    \n",
    "                    # wrap them in Variable\n",
    "                    if use_gpu:\n",
    "                        inputs = Variable(inputs.cuda())\n",
    "                        labels = Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / count\n",
    "            epoch_acc = running_corrects / count\n",
    "        \n",
    "            epoch_acc_dict[phase][epoch] = epoch_acc\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(confusion_matrix.conf)\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # store most recent val_loss\n",
    "            if phase == 'val':\n",
    "                val_acc_loss = (1.0 - epoch_acc)\n",
    "                print(\"val_acc_loss = \" + str(val_acc_loss))\n",
    "\n",
    "                        # save each epoch's model\n",
    "#             weights_path = \"resnet18_half_frozen_\" + str(epoch + 1) + \"epochs_transfer-state.pt\"\n",
    "#             torch.save(model_resnet18.state_dict(), weights_path)\n",
    "#             print(\"saved epoch \" + str(epoch + 1) + \" model state (weights) to \" + weights_path)\n",
    "#             print(\"ran epoch \" + str(epoch + 1))\n",
    "        \n",
    "        \n",
    "        # Extra spacing\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Include early stopping criteria check at end of epoch\n",
    "        if (early_stopping is not None) and early_stopping.checkStoppingCriteria(val_acc_loss):\n",
    "            print(\"Stopping after epoch \" + str(epoch) + \" due to early stopping criteria.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "\n",
    "all_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Set up data transforms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\Documents\\work\\BuildUCLA\\data\\printed_with_ids_harsh_filter\\preprocessed-images\n",
      "C:\\Users\\rahul\\Documents\\work\\BuildUCLA\\data\\printed_with_ids_harsh_filter\\book_number_mapping.csv\n",
      "C:\\Users\\rahul\\Documents\\work\\BuildUCLA\\data\\printed_with_ids_harsh_filter\\books-preprocessed-images\n"
     ]
    }
   ],
   "source": [
    "# Some data configuration, like data directory and transforms\n",
    "\n",
    "# data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids\\\\images\"\n",
    "data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\preprocessed-images\"\n",
    "meta_data_loc = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\book_number_mapping.csv\"\n",
    "book_data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\books-preprocessed-images\"\n",
    "\n",
    "print(data_dir)\n",
    "print(meta_data_loc)\n",
    "print(book_data_dir)\n",
    "\n",
    "data_transforms = selected_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data sets/loaders for each book\n",
    "\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "# test books are currently arbitrarily set\n",
    "test_books = set([\"Albin\", \"Dryden\"])\n",
    "# test_books = [\"Dryden\"]\n",
    "\n",
    "# Get the list of all books in the data set\n",
    "books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                 if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((1000,1000)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# Create a dict of datasets for each book\n",
    "book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), transform = data_transforms[t])#, transform=test_transform)\n",
    "                      for t in set_types}\n",
    "                 for b in books_in_data}\n",
    "\n",
    "# Create the test data loader, which will only have the books in test_books\n",
    "# book_data_loaders = {}\n",
    "# book_data_loaders[\"test\"] = {b : torch.utils.data.DataLoader(book_data_sets[b][\"test\"],\n",
    "#                                                         batch_size=4,\n",
    "#                                                         num_workers=4)\n",
    "#                         for b in test_books}\n",
    "\n",
    "# for each cross-validation, the \"train\" and \"val\" parts of the data_loaders dict will be modified accordingly\n",
    "\n",
    "\n",
    "\n",
    "# class_names = book_data_sets[\"Albin\"][\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# i = 0\n",
    "# for data in book_data_loaders[\"test\"][\"Dryden\"]:\n",
    "#     print(i)\n",
    "#     inputs, classes = data\n",
    "#     if i == 10:\n",
    "#         for ind in range(len(inputs)):\n",
    "#             img = inputs[ind]\n",
    "#             nimg = img.numpy().T\n",
    "#             imgplot = plt.imshow(nimg)\n",
    "#             print(classes[ind])\n",
    "#         break\n",
    "#     else:\n",
    "#         i += 1\n",
    "\n",
    "# print(\"visualized a few images\")\n",
    "\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read meta data of the images, and make book-index mapping dictionary\n",
    "\n",
    "# Assumes meta data file:\n",
    "#   - has field names in first row\n",
    "#   - the field names are in this order: [\"id\", \"book\", \"label\"]\n",
    "\n",
    "# Create a dict of dicts:\n",
    "#   - keys are based on the value in \"label\"\n",
    "#   - values are the \"inner\" dicts described below\n",
    "# Each \"inner\" dict has:\n",
    "#   - keys are the book names, specified in \"book\"\n",
    "#   - each value is a list of all the indices denoted by \"id\"\n",
    "\n",
    "# Below is commented temporarily\n",
    "# book_mapping = {}\n",
    "\n",
    "# with open(meta_data_loc, mode='r') as infile:\n",
    "#     reader = csv.DictReader(infile)\n",
    "#     for row in reader:\n",
    "#         if row[\"label\"] not in book_mapping:\n",
    "#             book_mapping[row[\"label\"]] = {}\n",
    "#         bm = book_mapping[row[\"label\"]]\n",
    "        \n",
    "#         if row[\"book\"] not in bm:\n",
    "#             bm[row[\"book\"]] = []\n",
    "#         bm[row[\"book\"]].append(int(row[\"id\"]))\n",
    "\n",
    "# count = 0\n",
    "# for l in book_mapping:\n",
    "#     for b in book_mapping[l]:\n",
    "#         count += len(book_mapping[l][b])\n",
    "\n",
    "# print(\"read meta data for \" + str(count) + \" images from \" + str(meta_data_loc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def valid_imshow_data(data):\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim == 2:\n",
    "        return True\n",
    "    elif data.ndim == 3:\n",
    "        if 3 <= data.shape[2] <= 4:\n",
    "            return True\n",
    "        else:\n",
    "            print('The \"data\" has 3 dimensions but the last dimension '\n",
    "                  'must have a length of 3 (RGB) or 4 (RGBA), not \"{}\".'\n",
    "                  ''.format(data.shape[2]))\n",
    "            return False\n",
    "    else:\n",
    "        print('To visualize an image the data must be 2 dimensional or '\n",
    "              '3 dimensional, not \"{}\".'\n",
    "              ''.format(data.ndim))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# set_types = ['train', 'val', 'test']\n",
    "# # test books are currently arbitrarily set\n",
    "# # test_books = [\"Albin\", \"Dryden\"]\n",
    "# test_books = [\"Dryden\"]\n",
    "\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((1000,1000)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# # Need to split data sets into training and testing sets\n",
    "# data_sets = {t : datasets.ImageFolder(data_dir, transform = test_transform)#, transform=data_transforms[t])\n",
    "#              for t in set_types}\n",
    "\n",
    "\n",
    "# data_samplers = {}\n",
    "# data_samplers[\"test\"] = sum([bm[b] for bm in book_mapping.values() for b in bm if b in test_books], [])\n",
    "# data_samplers[\"train\"] = []\n",
    "# data_samplers[\"val\"] = []\n",
    "\n",
    "\n",
    "# dataloaders = {t : torch.utils.data.DataLoader(data_sets[t],\n",
    "#                                               sampler=data_samplers[t],\n",
    "#                                               batch_size=4,\n",
    "#                                               num_workers=4)\n",
    "#               for t in set_types}\n",
    "\n",
    "# class_names = data_sets[\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# print(sorted(data_samplers[\"test\"]))\n",
    "\n",
    "# # inputs, classes = next(iter(dataloaders[\"test\"]))\n",
    "# i = 0\n",
    "# for inputs, classes in iter(dataloaders[\"test\"]):\n",
    "#     print(i)\n",
    "#     if i == 0:\n",
    "#         for img in inputs:\n",
    "#             nimg = img.numpy().T\n",
    "#             imgplot = plt.imshow(nimg)\n",
    "#         break\n",
    "#     else:\n",
    "#         i += 1\n",
    "\n",
    "# # inputs, classes = next(iter(dataloaders[\"test\"]))\n",
    "# # out = torchvision.utils.make_grid(inputs)\n",
    "# # imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "\n",
    "# # print(data_samplers)\n",
    "\n",
    "# # Get a batch of training data\n",
    "# # inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # # Make a grid from batch\n",
    "# # out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# # imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# print(\"visualized a few images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# set_types = ['train', 'val', 'test']\n",
    "\n",
    "# # Need to split data sets into training and testing sets\n",
    "# data_sets = {t : datasets.ImageFolder(data_dir, transform=data_transforms[t])\n",
    "#              for t in set_types}\n",
    "\n",
    "# # Need to split data into training set, validation set, and testing set\n",
    "# train_size = 0.65     # 65% of all data is training\n",
    "# val_size = 0.15       # 15% of all data is validation\n",
    "# test_size = (1 - train_size - val_size)    # Remaining data (20%) is testing\n",
    "\n",
    "# num_images = len(data_sets[\"train\"]) # length of both sets should be the same\n",
    "# all_ind = list(range(num_images))\n",
    "# random_seed = 11\n",
    "# np.random.seed(random_seed)\n",
    "# np.random.shuffle(all_ind)\n",
    "\n",
    "# train_split = int(num_images * train_size)\n",
    "# val_split = int(num_images * val_size)\n",
    "# test_split = int(num_images * test_size)\n",
    "\n",
    "# data_samplers = {}\n",
    "# data_samplers[\"train\"] = all_ind[:train_split]\n",
    "# data_samplers[\"val\"] = all_ind[train_split : train_split+val_split]\n",
    "# data_samplers[\"test\"] = all_ind[train_split+val_split:]\n",
    "\n",
    "# dataloaders = {t : torch.utils.data.DataLoader(data_sets[t],\n",
    "#                                               sampler=data_samplers[t],\n",
    "#                                               batch_size=4,\n",
    "#                                               num_workers=4)\n",
    "#               for t in set_types}\n",
    "\n",
    "# dataset_sizes = {t : len(data_samplers[t]) for t in set_types}\n",
    "\n",
    "# class_names = data_sets[\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# print(\"data loaded from \" + str(data_dir))\n",
    "# print(\"read classes: \" + str(class_names))\n",
    "# if use_gpu:\n",
    "#     print(\"use_gpu is true\")\n",
    "# else:\n",
    "#     print(\"use_gpu is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# print(\"visualized a few images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined function to build model architecture.\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture():\n",
    "    print(\"Transferring resnet18 and retraining with annotations dataset.\")\n",
    "\n",
    "    model_resnet18 = models.resnet18(pretrained=True)\n",
    "    num_params = sum(1 for i in model_resnet18.parameters())\n",
    "\n",
    "    # There are 10 layers (model_ft.children()) in resnet18\n",
    "    # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "    max_layer = 5\n",
    "    curr_layer = 1\n",
    "    last_layer = None\n",
    "    for child in model_resnet18.children():\n",
    "        if curr_layer <= max_layer:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            last_layer = child\n",
    "            curr_layer = curr_layer + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # for child in model.children():\n",
    "    #     print(\"\")\n",
    "    #     print(child)\n",
    "\n",
    "    # Replace the final fully connected layer to perform binary classification\n",
    "    num_ftrs = model_resnet18.fc.in_features\n",
    "    model_resnet18.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_resnet18 = model_resnet18.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model_resnet18.parameters())), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Create LR scheduler that decays LR by a factor of 0.1 for every 7 epochs (this is from tutorial, might need tweaking)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    return model_resnet18\n",
    "\n",
    "print(\"Defined function to build model architecture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform training\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.9302 Acc: 0.8061\n",
      "[[ 425  207]\n",
      " [ 210 1309]]\n",
      "\n",
      "val Loss: 2.2109 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Training complete in 1m 26s\n",
      "Best val Acc: 0.806597\n",
      "{'Montaigne': {0: 0.806136680613668}}\n",
      "{'Montaigne': {0: 0.8065967016491754}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 2.2559 Acc: 0.7687\n",
      "[[ 420  305]\n",
      " [ 271 1494]]\n",
      "\n",
      "val Loss: 3.3355 Acc: 0.5854\n",
      "[[ 10  26]\n",
      " [110 182]]\n",
      "\n",
      "val_acc_loss = 0.41463414634146345\n",
      "\n",
      "\n",
      "Training complete in 1m 26s\n",
      "Best val Acc: 0.585366\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Blackmore': {0: 0.7686746987951807}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Blackmore': {0: 0.5853658536585366}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.8996 Acc: 0.8188\n",
      "[[ 365  253]\n",
      " [ 226 1800]]\n",
      "\n",
      "val Loss: 7.8683 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Training complete in 1m 38s\n",
      "Best val Acc: 0.178161\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Albin': {0: 0.8188350983358548}, 'Blackmore': {0: 0.7686746987951807}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Albin': {0: 0.1781609195402299}, 'Blackmore': {0: 0.5853658536585366}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.8495 Acc: 0.8093\n",
      "[[ 466  275]\n",
      " [ 244 1737]]\n",
      "\n",
      "val Loss: 0.9620 Acc: 0.7917\n",
      "[[ 0 20]\n",
      " [ 0 76]]\n",
      "\n",
      "val_acc_loss = 0.20833333333333337\n",
      "\n",
      "\n",
      "Training complete in 1m 38s\n",
      "Best val Acc: 0.791667\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Albin': {0: 0.8188350983358548}, 'Ascham': {0: 0.8093313739897134}, 'Blackmore': {0: 0.7686746987951807}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Albin': {0: 0.1781609195402299}, 'Ascham': {0: 0.7916666666666666}, 'Blackmore': {0: 0.5853658536585366}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.9491 Acc: 0.8160\n",
      "[[ 491  264]\n",
      " [ 223 1669]]\n",
      "\n",
      "val Loss: 0.3364 Acc: 0.9649\n",
      "[[  0   6]\n",
      " [  0 165]]\n",
      "\n",
      "val_acc_loss = 0.03508771929824561\n",
      "\n",
      "\n",
      "Training complete in 1m 26s\n",
      "Best val Acc: 0.964912\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Albin': {0: 0.8188350983358548}, 'Ascham': {0: 0.8093313739897134}, 'Confucius': {0: 0.8160181337363053}, 'Blackmore': {0: 0.7686746987951807}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Albin': {0: 0.1781609195402299}, 'Ascham': {0: 0.7916666666666666}, 'Confucius': {0: 0.9649122807017544}, 'Blackmore': {0: 0.5853658536585366}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.5651 Acc: 0.8295\n",
      "[[ 469  226]\n",
      " [ 207 1638]]\n",
      "\n",
      "val Loss: 2.5106 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Training complete in 1m 37s\n",
      "Best val Acc: 0.762590\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Allestree': {0: 0.8295275590551181}, 'Blackmore': {0: 0.7686746987951807}, 'Albin': {0: 0.8188350983358548}, 'Ascham': {0: 0.8093313739897134}, 'Confucius': {0: 0.8160181337363053}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Allestree': {0: 0.762589928057554}, 'Blackmore': {0: 0.5853658536585366}, 'Albin': {0: 0.1781609195402299}, 'Ascham': {0: 0.7916666666666666}, 'Confucius': {0: 0.9649122807017544}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.6651 Acc: 0.8263\n",
      "[[ 374  227]\n",
      " [ 200 1657]]\n",
      "\n",
      "val Loss: 5.1169 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Training complete in 1m 30s\n",
      "Best val Acc: 0.555556\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Allestree': {0: 0.8295275590551181}, 'Blackmore': {0: 0.7686746987951807}, 'Browne': {0: 0.8262815296989422}, 'Albin': {0: 0.8188350983358548}, 'Ascham': {0: 0.8093313739897134}, 'Confucius': {0: 0.8160181337363053}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Allestree': {0: 0.762589928057554}, 'Blackmore': {0: 0.5853658536585366}, 'Browne': {0: 0.5555555555555556}, 'Albin': {0: 0.1781609195402299}, 'Ascham': {0: 0.7916666666666666}, 'Confucius': {0: 0.9649122807017544}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.9456 Acc: 0.8263\n",
      "[[ 406  248]\n",
      " [ 183 1645]]\n",
      "\n",
      "val Loss: 2.4862 Acc: 0.4583\n",
      "[[ 67  40]\n",
      " [142  87]]\n",
      "\n",
      "val_acc_loss = 0.5416666666666667\n",
      "\n",
      "\n",
      "Training complete in 1m 35s\n",
      "Best val Acc: 0.458333\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Allestree': {0: 0.8295275590551181}, 'Blackmore': {0: 0.7686746987951807}, 'Browne': {0: 0.8262815296989422}, 'Albin': {0: 0.8188350983358548}, 'Ascham': {0: 0.8093313739897134}, 'Confucius': {0: 0.8160181337363053}, 'Defoe': {0: 0.8263497179693795}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Allestree': {0: 0.762589928057554}, 'Blackmore': {0: 0.5853658536585366}, 'Browne': {0: 0.5555555555555556}, 'Albin': {0: 0.1781609195402299}, 'Ascham': {0: 0.7916666666666666}, 'Confucius': {0: 0.9649122807017544}, 'Defoe': {0: 0.4583333333333333}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 1.7379 Acc: 0.8274\n",
      "[[ 470  252]\n",
      " [ 226 1822]]\n",
      "\n",
      "val Loss: 6.6066 Acc: 0.1875\n",
      "[[ 0 39]\n",
      " [ 0  9]]\n",
      "\n",
      "val_acc_loss = 0.8125\n",
      "\n",
      "\n",
      "Training complete in 1m 33s\n",
      "Best val Acc: 0.187500\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Allestree': {0: 0.8295275590551181}, 'Blackmore': {0: 0.7686746987951807}, 'Browne': {0: 0.8262815296989422}, 'Albin': {0: 0.8188350983358548}, 'Ascham': {0: 0.8093313739897134}, 'Confucius': {0: 0.8160181337363053}, 'Defoe': {0: 0.8263497179693795}, 'Dryden': {0: 0.8274368231046931}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Allestree': {0: 0.762589928057554}, 'Blackmore': {0: 0.5853658536585366}, 'Browne': {0: 0.5555555555555556}, 'Albin': {0: 0.1781609195402299}, 'Ascham': {0: 0.7916666666666666}, 'Confucius': {0: 0.9649122807017544}, 'Defoe': {0: 0.4583333333333333}, 'Dryden': {0: 0.1875}}\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'train', 'val'])\n",
      "Epoch 0/0\n",
      "----------\n",
      "train Loss: 2.2064 Acc: 0.7860\n",
      "[[ 424  282]\n",
      " [ 244 1508]]\n",
      "\n",
      "val Loss: 0.5195 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Training complete in 1m 31s\n",
      "Best val Acc: 0.847222\n",
      "{'Montaigne': {0: 0.806136680613668}, 'Allestree': {0: 0.8295275590551181}, 'Blackmore': {0: 0.7686746987951807}, 'Browne': {0: 0.8262815296989422}, 'Albin': {0: 0.8188350983358548}, 'Voltaire': {0: 0.7860048820179008}, 'Ascham': {0: 0.8093313739897134}, 'Confucius': {0: 0.8160181337363053}, 'Defoe': {0: 0.8263497179693795}, 'Dryden': {0: 0.8274368231046931}}\n",
      "{'Montaigne': {0: 0.8065967016491754}, 'Allestree': {0: 0.762589928057554}, 'Blackmore': {0: 0.5853658536585366}, 'Browne': {0: 0.5555555555555556}, 'Albin': {0: 0.1781609195402299}, 'Voltaire': {0: 0.8472222222222222}, 'Ascham': {0: 0.7916666666666666}, 'Confucius': {0: 0.9649122807017544}, 'Defoe': {0: 0.4583333333333333}, 'Dryden': {0: 0.1875}}\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "print(\"perform training\")\n",
    "\n",
    "# Validation scores for determining the best number of epochs\n",
    "#    - The keys are the book names, and point to dicts that are indexed by epoch number\n",
    "all_epoch_scores = {\"train\" : {}, \"val\" : {}}\n",
    "\n",
    "num_training_epochs = 50\n",
    "\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # setup this cross val's data loaders\n",
    "    train_books = books_in_data - set([val_book])\n",
    "    \n",
    "    book_data_loaders = {}\n",
    "    book_data_loaders = {\"test\" :\n",
    "                         {b : torch.utils.data.DataLoader(book_data_sets[b][\"test\"],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for b in test_books},\n",
    "                         \"train\" :\n",
    "                         {b : torch.utils.data.DataLoader(book_data_sets[b][\"train\"],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for b in train_books},\n",
    "                         \"val\" :\n",
    "                         {b : torch.utils.data.DataLoader(book_data_sets[b][\"val\"],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for b in [val_book]}\n",
    "                        }\n",
    "    \n",
    "    # can customize this later to transfer from different models, freezing different numbers of layers, etc.\n",
    "    model_architecture = create_model_architecture()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p:\n",
    "                                      p.requires_grad, model_architecture.parameters())),\n",
    "                          lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Create LR scheduler that decays LR by a factor of 0.1 for every 7 epochs (this is from tutorial, might need tweaking)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "    \n",
    "    print(book_data_loaders.keys())\n",
    "\n",
    "    trained_model_weights, epoch_scores = train_model(model_architecture,\n",
    "                                                      criterion,\n",
    "                                                      optimizer,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      book_data_loaders,\n",
    "                                                      num_epochs=num_training_epochs,\n",
    "                                                      early_stopping = earlyStoppingCriteria)\n",
    "    for t in epoch_scores:\n",
    "        all_epoch_scores[t][val_book] = epoch_scores[t]\n",
    "    \n",
    "# Average scores over the books for each epoch\n",
    "ave_val_scores = {t : {} for t in all_epoch_scores}\n",
    "for t in all_epoch_scores:\n",
    "    for epoch in range(num_training_epochs):\n",
    "        count = 0\n",
    "        cum = 0\n",
    "        for book in all_epoch_scores[t]:\n",
    "            if epoch in all_epoch_scores[t][book]:\n",
    "                cum += all_epoch_scores[t][book][epoch]\n",
    "                count += 1\n",
    "        ave_val_scores[t][epoch] = (cum/count)\n",
    "\n",
    "print(ave_val_scores)\n",
    "\n",
    "best_epochs = {t : max(ave_val_scores[t], key=ave_val_scores.get) for t in ave_val_scores}\n",
    "print(best_epochs)\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = \"resnet18_half_frozen_5epochs_transfer-state.pt\"\n",
    "torch.save(model_resnet18.state_dict(), weights_path)\n",
    "print(\"saved model state (weights) to \" + weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['test']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_resnet18)\n",
    "print(\"visualizing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"running on testing dataset\")\n",
    "model_resnet18.train(False)  # Set model to evaluate mode\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over data.\n",
    "for data in dataloaders[\"test\"]:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    outputs = model_resnet18(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # backward + optimize only if in training phase\n",
    "#     if phase == 'train':\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / dataset_sizes[\"test\"]\n",
    "epoch_acc = running_corrects / dataset_sizes[\"test\"]\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "    \"test\", epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
