{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 1 with resnet18 transfer learning.\n"
     ]
    }
   ],
   "source": [
    "print(\"Experiment 1 with resnet18 transfer learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imports 1 complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import torchnet as tnt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "# Disabled for memory issues\n",
    "# use_gpu = False\n",
    "\n",
    "print(\"imports 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a class to deal with early stopping criteria\n",
    "# Important: minimizes a loss value\n",
    "class EarlyStopping:\n",
    "    def __init__(self, min_delta=0, patience=5):\n",
    "        # The minimum delta in loss to be considered a change in loss\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        # number of epochs to wait for improvement before terminating\n",
    "        self.patience = patience\n",
    "        \n",
    "        # number of epochs waited\n",
    "        self.wait = 0\n",
    "        \n",
    "        # Set \"best loss\" to some large number\n",
    "        self.best_loss = 1e15\n",
    "        \n",
    "    def checkStoppingCriteria(self, curr_loss):\n",
    "        \"\"\" Returns whether the stopping criteria has been met. \"\"\"\n",
    "        if (curr_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = curr_loss\n",
    "            self.wait = 1\n",
    "        elif self.wait < self.patience:\n",
    "            self.wait += 1\n",
    "        else:\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This 'train_model' function is a generic routine that can be used to train various models.\n"
     ]
    }
   ],
   "source": [
    "print(\"This 'train_model' function is a generic routine that can be used to train various models.\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, data_loaders, num_epochs=25, early_stopping = None):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    val_acc_loss = 0.0\n",
    "    \n",
    "#     epoch_val_accs = {}\n",
    "#     epoch_train_accs = {}\n",
    "    epoch_acc_dict = {\"train\": {}, \"val\" : {}}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Confusion matrix\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "            \n",
    "            count = 0\n",
    "            \n",
    "            # Iterate over each book\n",
    "            for book in data_loaders[phase]:\n",
    "                \n",
    "                # Iterate over data.\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                    count += len(inputs)\n",
    "                    \n",
    "                    # wrap them in Variable\n",
    "                    if use_gpu:\n",
    "                        inputs = Variable(inputs.cuda())\n",
    "                        labels = Variable(labels.cuda())\n",
    "#                         inputs = Variable(inputs, volatile=True).cuda()\n",
    "#                         labels = Variable(labels, volatile=True).cuda()\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / count\n",
    "            epoch_acc = running_corrects / count\n",
    "        \n",
    "            epoch_acc_dict[phase][epoch] = epoch_acc\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(confusion_matrix.conf)\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # store most recent val_loss\n",
    "            if phase == 'val':\n",
    "                val_acc_loss = (1.0 - epoch_acc)\n",
    "                print(\"val_acc_loss = \" + str(val_acc_loss))\n",
    "\n",
    "                        # save each epoch's model\n",
    "#             weights_path = \"resnet18_half_frozen_\" + str(epoch + 1) + \"epochs_transfer-state.pt\"\n",
    "#             torch.save(model_resnet18.state_dict(), weights_path)\n",
    "#             print(\"saved epoch \" + str(epoch + 1) + \" model state (weights) to \" + weights_path)\n",
    "#             print(\"ran epoch \" + str(epoch + 1))\n",
    "        \n",
    "        \n",
    "        # Extra spacing\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Include early stopping criteria check at end of epoch\n",
    "        if (early_stopping is not None) and early_stopping.checkStoppingCriteria(val_acc_loss):\n",
    "            print(\"Stopping after epoch \" + str(epoch) + \" due to early stopping criteria.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "\n",
    "all_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Set up data transforms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\Documents\\work\\BuildUCLA\\data\\printed_with_ids_harsh_filter\\preprocessed-images\n",
      "C:\\Users\\rahul\\Documents\\work\\BuildUCLA\\data\\printed_with_ids_harsh_filter\\book_number_mapping.csv\n",
      "C:\\Users\\rahul\\Documents\\work\\BuildUCLA\\data\\printed_with_ids_harsh_filter\\books-preprocessed-images\n"
     ]
    }
   ],
   "source": [
    "# Some data configuration, like data directory and transforms\n",
    "\n",
    "# data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids\\\\images\"\n",
    "data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\preprocessed-images\"\n",
    "meta_data_loc = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\book_number_mapping.csv\"\n",
    "book_data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\books-preprocessed-images\"\n",
    "\n",
    "print(data_dir)\n",
    "print(meta_data_loc)\n",
    "print(book_data_dir)\n",
    "\n",
    "data_transforms = selected_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "# Create data sets/loaders for each book\n",
    "\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "# test books are currently arbitrarily set\n",
    "test_books = set([\"Albin\", \"Dryden\"])\n",
    "# test_books = [\"Dryden\"]\n",
    "\n",
    "# Get the list of all books in the data set\n",
    "books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                 if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((1000,1000)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# Create a dict of datasets for each book\n",
    "book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), transform = data_transforms[t])#, transform=test_transform)\n",
    "                      for t in set_types}\n",
    "                 for b in books_in_data}\n",
    "\n",
    "book_data_loaders = {b : {t : torch.utils.data.DataLoader(book_data_sets[b][t],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for t in set_types}\n",
    "                     for b in books_in_data}\n",
    "\n",
    "print(\"loaded data\")\n",
    "\n",
    "# book_data_loaders = {b : torch.utils.data.DataLoader(book_data_sets[b][\"train\"],\n",
    "#                                                                   batch_size=4,\n",
    "#                                                                   num_workers=1)\n",
    "#                                   for b in train_books}\n",
    "\n",
    "# Create the test data loader, which will only have the books in test_books\n",
    "# book_data_loaders = {}\n",
    "# book_data_loaders[\"test\"] = {b : torch.utils.data.DataLoader(book_data_sets[b][\"test\"],\n",
    "#                                                         batch_size=4,\n",
    "#                                                         num_workers=4)\n",
    "#                         for b in test_books}\n",
    "\n",
    "# for each cross-validation, the \"train\" and \"val\" parts of the data_loaders dict will be modified accordingly\n",
    "\n",
    "\n",
    "\n",
    "# class_names = book_data_sets[\"Albin\"][\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# i = 0\n",
    "# for data in book_data_loaders[\"test\"][\"Dryden\"]:\n",
    "#     print(i)\n",
    "#     inputs, classes = data\n",
    "#     if i == 10:\n",
    "#         for ind in range(len(inputs)):\n",
    "#             img = inputs[ind]\n",
    "#             nimg = img.numpy().T\n",
    "#             imgplot = plt.imshow(nimg)\n",
    "#             print(classes[ind])\n",
    "#         break\n",
    "#     else:\n",
    "#         i += 1\n",
    "\n",
    "# print(\"visualized a few images\")\n",
    "\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read meta data of the images, and make book-index mapping dictionary\n",
    "\n",
    "# Assumes meta data file:\n",
    "#   - has field names in first row\n",
    "#   - the field names are in this order: [\"id\", \"book\", \"label\"]\n",
    "\n",
    "# Create a dict of dicts:\n",
    "#   - keys are based on the value in \"label\"\n",
    "#   - values are the \"inner\" dicts described below\n",
    "# Each \"inner\" dict has:\n",
    "#   - keys are the book names, specified in \"book\"\n",
    "#   - each value is a list of all the indices denoted by \"id\"\n",
    "\n",
    "# Below is commented temporarily\n",
    "# book_mapping = {}\n",
    "\n",
    "# with open(meta_data_loc, mode='r') as infile:\n",
    "#     reader = csv.DictReader(infile)\n",
    "#     for row in reader:\n",
    "#         if row[\"label\"] not in book_mapping:\n",
    "#             book_mapping[row[\"label\"]] = {}\n",
    "#         bm = book_mapping[row[\"label\"]]\n",
    "        \n",
    "#         if row[\"book\"] not in bm:\n",
    "#             bm[row[\"book\"]] = []\n",
    "#         bm[row[\"book\"]].append(int(row[\"id\"]))\n",
    "\n",
    "# count = 0\n",
    "# for l in book_mapping:\n",
    "#     for b in book_mapping[l]:\n",
    "#         count += len(book_mapping[l][b])\n",
    "\n",
    "# print(\"read meta data for \" + str(count) + \" images from \" + str(meta_data_loc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def valid_imshow_data(data):\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim == 2:\n",
    "        return True\n",
    "    elif data.ndim == 3:\n",
    "        if 3 <= data.shape[2] <= 4:\n",
    "            return True\n",
    "        else:\n",
    "            print('The \"data\" has 3 dimensions but the last dimension '\n",
    "                  'must have a length of 3 (RGB) or 4 (RGBA), not \"{}\".'\n",
    "                  ''.format(data.shape[2]))\n",
    "            return False\n",
    "    else:\n",
    "        print('To visualize an image the data must be 2 dimensional or '\n",
    "              '3 dimensional, not \"{}\".'\n",
    "              ''.format(data.ndim))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# set_types = ['train', 'val', 'test']\n",
    "# # test books are currently arbitrarily set\n",
    "# # test_books = [\"Albin\", \"Dryden\"]\n",
    "# test_books = [\"Dryden\"]\n",
    "\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((1000,1000)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# # Need to split data sets into training and testing sets\n",
    "# data_sets = {t : datasets.ImageFolder(data_dir, transform = test_transform)#, transform=data_transforms[t])\n",
    "#              for t in set_types}\n",
    "\n",
    "\n",
    "# data_samplers = {}\n",
    "# data_samplers[\"test\"] = sum([bm[b] for bm in book_mapping.values() for b in bm if b in test_books], [])\n",
    "# data_samplers[\"train\"] = []\n",
    "# data_samplers[\"val\"] = []\n",
    "\n",
    "\n",
    "# dataloaders = {t : torch.utils.data.DataLoader(data_sets[t],\n",
    "#                                               sampler=data_samplers[t],\n",
    "#                                               batch_size=4,\n",
    "#                                               num_workers=4)\n",
    "#               for t in set_types}\n",
    "\n",
    "# class_names = data_sets[\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# print(sorted(data_samplers[\"test\"]))\n",
    "\n",
    "# # inputs, classes = next(iter(dataloaders[\"test\"]))\n",
    "# i = 0\n",
    "# for inputs, classes in iter(dataloaders[\"test\"]):\n",
    "#     print(i)\n",
    "#     if i == 0:\n",
    "#         for img in inputs:\n",
    "#             nimg = img.numpy().T\n",
    "#             imgplot = plt.imshow(nimg)\n",
    "#         break\n",
    "#     else:\n",
    "#         i += 1\n",
    "\n",
    "# # inputs, classes = next(iter(dataloaders[\"test\"]))\n",
    "# # out = torchvision.utils.make_grid(inputs)\n",
    "# # imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "\n",
    "# # print(data_samplers)\n",
    "\n",
    "# # Get a batch of training data\n",
    "# # inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # # Make a grid from batch\n",
    "# # out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# # imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# print(\"visualized a few images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# set_types = ['train', 'val', 'test']\n",
    "\n",
    "# # Need to split data sets into training and testing sets\n",
    "# data_sets = {t : datasets.ImageFolder(data_dir, transform=data_transforms[t])\n",
    "#              for t in set_types}\n",
    "\n",
    "# # Need to split data into training set, validation set, and testing set\n",
    "# train_size = 0.65     # 65% of all data is training\n",
    "# val_size = 0.15       # 15% of all data is validation\n",
    "# test_size = (1 - train_size - val_size)    # Remaining data (20%) is testing\n",
    "\n",
    "# num_images = len(data_sets[\"train\"]) # length of both sets should be the same\n",
    "# all_ind = list(range(num_images))\n",
    "# random_seed = 11\n",
    "# np.random.seed(random_seed)\n",
    "# np.random.shuffle(all_ind)\n",
    "\n",
    "# train_split = int(num_images * train_size)\n",
    "# val_split = int(num_images * val_size)\n",
    "# test_split = int(num_images * test_size)\n",
    "\n",
    "# data_samplers = {}\n",
    "# data_samplers[\"train\"] = all_ind[:train_split]\n",
    "# data_samplers[\"val\"] = all_ind[train_split : train_split+val_split]\n",
    "# data_samplers[\"test\"] = all_ind[train_split+val_split:]\n",
    "\n",
    "# dataloaders = {t : torch.utils.data.DataLoader(data_sets[t],\n",
    "#                                               sampler=data_samplers[t],\n",
    "#                                               batch_size=4,\n",
    "#                                               num_workers=4)\n",
    "#               for t in set_types}\n",
    "\n",
    "# dataset_sizes = {t : len(data_samplers[t]) for t in set_types}\n",
    "\n",
    "# class_names = data_sets[\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# print(\"data loaded from \" + str(data_dir))\n",
    "# print(\"read classes: \" + str(class_names))\n",
    "# if use_gpu:\n",
    "#     print(\"use_gpu is true\")\n",
    "# else:\n",
    "#     print(\"use_gpu is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# print(\"visualized a few images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined function to build model architecture.\n"
     ]
    }
   ],
   "source": [
    "def create_model_architecture():\n",
    "    print(\"Transferring resnet18 and retraining with annotations dataset.\")\n",
    "\n",
    "    model_resnet18 = models.resnet18(pretrained=True)\n",
    "    num_params = sum(1 for i in model_resnet18.parameters())\n",
    "\n",
    "    # There are 10 layers (model_ft.children()) in resnet18\n",
    "    # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "    max_layer = 5\n",
    "    curr_layer = 1\n",
    "    last_layer = None\n",
    "    for child in model_resnet18.children():\n",
    "        if curr_layer <= max_layer:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            last_layer = child\n",
    "            curr_layer = curr_layer + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # for child in model.children():\n",
    "    #     print(\"\")\n",
    "    #     print(child)\n",
    "\n",
    "    # Replace the final fully connected layer to perform binary classification\n",
    "    num_ftrs = model_resnet18.fc.in_features\n",
    "    model_resnet18.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_resnet18 = model_resnet18.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model_resnet18.parameters())), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Create LR scheduler that decays LR by a factor of 0.1 for every 7 epochs (this is from tutorial, might need tweaking)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    return model_resnet18\n",
    "\n",
    "print(\"Defined function to build model architecture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform training\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'val', 'train'])\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.7240 Acc: 0.7644\n",
      "[[ 296  305]\n",
      " [ 274 1583]]\n",
      "\n",
      "val Loss: 5.5554 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.1598 Acc: 0.7661\n",
      "[[ 262  339]\n",
      " [ 236 1621]]\n",
      "\n",
      "val Loss: 5.0704 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.9628 Acc: 0.7600\n",
      "[[ 250  351]\n",
      " [ 239 1618]]\n",
      "\n",
      "val Loss: 3.6160 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.8782 Acc: 0.7543\n",
      "[[ 226  375]\n",
      " [ 229 1628]]\n",
      "\n",
      "val Loss: 3.1870 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.8382 Acc: 0.7535\n",
      "[[ 215  386]\n",
      " [ 220 1637]]\n",
      "\n",
      "val Loss: 2.9672 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.8250 Acc: 0.7510\n",
      "[[ 206  395]\n",
      " [ 217 1640]]\n",
      "\n",
      "val Loss: 3.4056 Acc: 0.5556\n",
      "[[  0 160]\n",
      " [  0 200]]\n",
      "\n",
      "val_acc_loss = 0.4444444444444444\n",
      "\n",
      "\n",
      "Stopping after epoch 5 due to early stopping criteria.\n",
      "Training complete in 13m 10s\n",
      "Best val Acc: 0.555556\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'val', 'train'])\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.8842 Acc: 0.7740\n",
      "[[ 396  299]\n",
      " [ 275 1570]]\n",
      "\n",
      "val Loss: 2.1515 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.0343 Acc: 0.7594\n",
      "[[ 344  351]\n",
      " [ 260 1585]]\n",
      "\n",
      "val Loss: 2.0974 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.9421 Acc: 0.7531\n",
      "[[ 323  372]\n",
      " [ 255 1590]]\n",
      "\n",
      "val Loss: 1.3823 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.9131 Acc: 0.7504\n",
      "[[ 312  383]\n",
      " [ 251 1594]]\n",
      "\n",
      "val Loss: 1.2419 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.8504 Acc: 0.7398\n",
      "[[ 283  412]\n",
      " [ 249 1596]]\n",
      "\n",
      "val Loss: 1.0064 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.8270 Acc: 0.7417\n",
      "[[ 275  420]\n",
      " [ 236 1609]]\n",
      "\n",
      "val Loss: 1.6660 Acc: 0.7626\n",
      "[[  0  66]\n",
      " [  0 212]]\n",
      "\n",
      "val_acc_loss = 0.237410071942446\n",
      "\n",
      "\n",
      "Stopping after epoch 5 due to early stopping criteria.\n",
      "Training complete in 13m 34s\n",
      "Best val Acc: 0.762590\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'val', 'train'])\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 2.0159 Acc: 0.7387\n",
      "[[ 329  303]\n",
      " [ 259 1260]]\n",
      "\n",
      "val Loss: 2.1873 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.1351 Acc: 0.7252\n",
      "[[ 289  343]\n",
      " [ 248 1271]]\n",
      "\n",
      "val Loss: 1.5093 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 1.0170 Acc: 0.7243\n",
      "[[ 284  348]\n",
      " [ 245 1274]]\n",
      "\n",
      "val Loss: 1.5598 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.9551 Acc: 0.7215\n",
      "[[ 269  363]\n",
      " [ 236 1283]]\n",
      "\n",
      "val Loss: 1.1210 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.9247 Acc: 0.7127\n",
      "[[ 255  377]\n",
      " [ 241 1278]]\n",
      "\n",
      "val Loss: 1.0911 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.8897 Acc: 0.7141\n",
      "[[ 237  395]\n",
      " [ 220 1299]]\n",
      "\n",
      "val Loss: 1.3378 Acc: 0.8066\n",
      "[[  0 129]\n",
      " [  0 538]]\n",
      "\n",
      "val_acc_loss = 0.1934032983508246\n",
      "\n",
      "\n",
      "Stopping after epoch 5 due to early stopping criteria.\n",
      "Training complete in 13m 3s\n",
      "Best val Acc: 0.806597\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'val', 'train'])\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.8606 Acc: 0.7829\n",
      "[[ 315  303]\n",
      " [ 271 1755]]\n",
      "\n",
      "val Loss: 9.0582 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.1255 Acc: 0.7500\n",
      "[[ 227  391]\n",
      " [ 270 1756]]\n",
      "\n",
      "val Loss: 8.2222 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.9479 Acc: 0.7439\n",
      "[[ 202  416]\n",
      " [ 261 1765]]\n",
      "\n",
      "val Loss: 5.1229 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.8557 Acc: 0.7534\n",
      "[[ 188  430]\n",
      " [ 222 1804]]\n",
      "\n",
      "val Loss: 6.7991 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.8173 Acc: 0.7496\n",
      "[[ 164  454]\n",
      " [ 208 1818]]\n",
      "\n",
      "val Loss: 4.6024 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.7998 Acc: 0.7424\n",
      "[[ 149  469]\n",
      " [ 212 1814]]\n",
      "\n",
      "val Loss: 6.9333 Acc: 0.1782\n",
      "[[  0 143]\n",
      " [  0  31]]\n",
      "\n",
      "val_acc_loss = 0.8218390804597702\n",
      "\n",
      "\n",
      "Stopping after epoch 5 due to early stopping criteria.\n",
      "Training complete in 14m 29s\n",
      "Best val Acc: 0.178161\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'val', 'train'])\n",
      "Epoch 0/49\n",
      "----------\n",
      "train Loss: 1.7879 Acc: 0.7710\n",
      "[[ 412  294]\n",
      " [ 269 1483]]\n",
      "\n",
      "val Loss: 1.3178 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Epoch 1/49\n",
      "----------\n",
      "train Loss: 1.0327 Acc: 0.7498\n",
      "[[ 370  336]\n",
      " [ 279 1473]]\n",
      "\n",
      "val Loss: 0.8332 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Epoch 2/49\n",
      "----------\n",
      "train Loss: 0.9322 Acc: 0.7469\n",
      "[[ 354  352]\n",
      " [ 270 1482]]\n",
      "\n",
      "val Loss: 1.0137 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Epoch 3/49\n",
      "----------\n",
      "train Loss: 0.8877 Acc: 0.7429\n",
      "[[ 332  374]\n",
      " [ 258 1494]]\n",
      "\n",
      "val Loss: 1.0495 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Epoch 4/49\n",
      "----------\n",
      "train Loss: 0.8472 Acc: 0.7392\n",
      "[[ 313  393]\n",
      " [ 248 1504]]\n",
      "\n",
      "val Loss: 0.6840 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Epoch 5/49\n",
      "----------\n",
      "train Loss: 0.8213 Acc: 0.7364\n",
      "[[ 292  414]\n",
      " [ 234 1518]]\n",
      "\n",
      "val Loss: 0.7697 Acc: 0.8472\n",
      "[[  0  55]\n",
      " [  0 305]]\n",
      "\n",
      "val_acc_loss = 0.1527777777777778\n",
      "\n",
      "\n",
      "Stopping after epoch 5 due to early stopping criteria.\n",
      "Training complete in 9m 11s\n",
      "Best val Acc: 0.847222\n",
      "Transferring resnet18 and retraining with annotations dataset.\n",
      "dict_keys(['test', 'val', 'train'])\n",
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 124, in __getitem__\n    img = self.transform(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 42, in __call__\n    img = t(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 579, in __call__\n    return transform(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 42, in __call__\n    img = t(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 232, in __call__\n    return self.lambd(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 562, in <lambda>\n    transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 485, in adjust_hue\n    np_h = np.array(h, dtype=np.uint8)\nMemoryError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-82342889dd7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m                                                       \u001b[0mcross_val_loaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m                                                       \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_training_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m                                                       early_stopping = earlyStoppingCriteria)\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;31m# Attempt to resolve memory issue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-55ed9b74103b>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, criterion, optimizer, scheduler, data_loaders, num_epochs, early_stopping)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                 \u001b[1;31m# Iterate over data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                     \u001b[1;31m# get the inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 42, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\datasets\\folder.py\", line 124, in __getitem__\n    img = self.transform(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 42, in __call__\n    img = t(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 579, in __call__\n    return transform(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 42, in __call__\n    img = t(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 232, in __call__\n    return self.lambd(img)\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 562, in <lambda>\n    transforms.append(Lambda(lambda img: F.adjust_hue(img, hue_factor)))\n  File \"C:\\Users\\rahul\\AppData\\Local\\conda\\conda\\envs\\py35\\lib\\site-packages\\torchvision\\transforms\\functional.py\", line 485, in adjust_hue\n    np_h = np.array(h, dtype=np.uint8)\nMemoryError\n"
     ]
    }
   ],
   "source": [
    "print(\"perform training\")\n",
    "\n",
    "# Validation scores for determining the best number of epochs\n",
    "#    - The keys are the book names, and point to dicts that are indexed by epoch number\n",
    "all_epoch_scores = {\"train\" : {}, \"val\" : {}}\n",
    "\n",
    "num_training_epochs = 50\n",
    "\n",
    "cross_val_loaders = {}\n",
    "cross_val_loaders[\"test\"] = {b : book_data_loaders[b][\"test\"] for b in test_books}\n",
    "\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # setup this cross val's data loaders\n",
    "    train_books = books_in_data - set([val_book])\n",
    "    \n",
    "    cross_val_loaders[\"train\"] = {b : book_data_loaders[b][\"train\"] for b in train_books}\n",
    "    cross_val_loaders[\"val\"] = {b : book_data_loaders[b][\"val\"] for b in [val_book]}\n",
    "    \n",
    "    # can customize this later to transfer from different models, freezing different numbers of layers, etc.\n",
    "    model_architecture = create_model_architecture()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p:\n",
    "                                      p.requires_grad, model_architecture.parameters())),\n",
    "                          lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Create LR scheduler that decays LR by a factor of 0.1 for every 7 epochs (this is from tutorial, might need tweaking)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "    \n",
    "    print(cross_val_loaders.keys())\n",
    "\n",
    "    trained_model_weights, epoch_scores = train_model(model_architecture,\n",
    "                                                      criterion,\n",
    "                                                      optimizer,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      cross_val_loaders,\n",
    "                                                      num_epochs=num_training_epochs,\n",
    "                                                      early_stopping = earlyStoppingCriteria)\n",
    "    # Attempt to resolve memory issue\n",
    "    gc.collect()\n",
    "    # testing for memory management\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for t in epoch_scores:\n",
    "        all_epoch_scores[t][val_book] = epoch_scores[t]\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Average scores over the books for each epoch\n",
    "ave_val_scores = {t : {} for t in all_epoch_scores}\n",
    "for t in all_epoch_scores:\n",
    "    for epoch in range(num_training_epochs):\n",
    "        count = 0\n",
    "        cum = 0\n",
    "        for book in all_epoch_scores[t]:\n",
    "            if epoch in all_epoch_scores[t][book]:\n",
    "                cum += all_epoch_scores[t][book][epoch]\n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            ave_val_scores[t][epoch] = (cum/count)\n",
    "\n",
    "print(ave_val_scores)\n",
    "\n",
    "best_epochs = {t : max(ave_val_scores[t], key=ave_val_scores[t].get) for t in ave_val_scores}\n",
    "print(best_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = \"resnet18_half_frozen_5epochs_transfer-state.pt\"\n",
    "torch.save(model_resnet18.state_dict(), weights_path)\n",
    "print(\"saved model state (weights) to \" + weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['test']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_resnet18)\n",
    "print(\"visualizing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"running on testing dataset\")\n",
    "model_resnet18.train(False)  # Set model to evaluate mode\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over data.\n",
    "for data in dataloaders[\"test\"]:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    outputs = model_resnet18(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # backward + optimize only if in training phase\n",
    "#     if phase == 'train':\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / dataset_sizes[\"test\"]\n",
    "epoch_acc = running_corrects / dataset_sizes[\"test\"]\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "    \"test\", epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
