{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Experiment 1 with resnet18 transfer learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "\n",
    "import torchnet as tnt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "print(\"imports 1 complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating a class to deal with early stopping criteria\n",
    "# Important: minimizes a loss value\n",
    "class EarlyStopping:\n",
    "    def __init__(self, min_delta=0, patience=5):\n",
    "        # The minimum delta in loss to be considered a change in loss\n",
    "        self.min_delta = min_delta\n",
    "        \n",
    "        # number of epochs to wait for improvement before terminating\n",
    "        self.patience = patience\n",
    "        \n",
    "        # number of epochs waited\n",
    "        self.wait = 0\n",
    "        \n",
    "        # Set \"best loss\" to some large number\n",
    "        self.best_loss = 1e15\n",
    "        \n",
    "    def checkStoppingCriteria(self, curr_loss):\n",
    "        \"\"\" Returns whether the stopping criteria has been met. \"\"\"\n",
    "        if (curr_loss - self.best_loss) < -self.min_delta:\n",
    "            self.best_loss = curr_loss\n",
    "            self.wait = 1\n",
    "        elif self.wait < self.patience:\n",
    "            self.wait += 1\n",
    "        else:\n",
    "            return True\n",
    "        return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"This 'train_model' function is a generic routine that can be used to train various models.\")\n",
    "\n",
    "def train_model(model, criterion, optimizer, scheduler, data_loaders, num_epochs=25, early_stopping = None):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    \n",
    "    val_acc_loss = 0.0\n",
    "    \n",
    "#     epoch_val_accs = {}\n",
    "#     epoch_train_accs = {}\n",
    "    epoch_acc_dict = {\"train\": {}, \"val\" : {}}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Confusion matrix\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "            \n",
    "            count = 0\n",
    "            \n",
    "            # Iterate over each book\n",
    "            for book in data_loaders[phase]:\n",
    "                \n",
    "                # Iterate over data.\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                    count += len(inputs)\n",
    "                    \n",
    "                    # wrap them in Variable\n",
    "                    if use_gpu:\n",
    "                        inputs = Variable(inputs.cuda())\n",
    "                        labels = Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / count\n",
    "            epoch_acc = running_corrects / count\n",
    "        \n",
    "            epoch_acc_dict[phase][epoch] = epoch_acc\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(confusion_matrix.conf)\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # store most recent val_loss\n",
    "            if phase == 'val':\n",
    "                val_acc_loss = (1.0 - epoch_acc)\n",
    "                print(\"val_acc_loss = \" + str(val_acc_loss))\n",
    "\n",
    "                        # save each epoch's model\n",
    "#             weights_path = \"resnet18_half_frozen_\" + str(epoch + 1) + \"epochs_transfer-state.pt\"\n",
    "#             torch.save(model_resnet18.state_dict(), weights_path)\n",
    "#             print(\"saved epoch \" + str(epoch + 1) + \" model state (weights) to \" + weights_path)\n",
    "#             print(\"ran epoch \" + str(epoch + 1))\n",
    "        \n",
    "        \n",
    "        # Extra spacing\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Include early stopping criteria check at end of epoch\n",
    "        if (early_stopping is not None) and early_stopping.checkStoppingCriteria(val_acc_loss):\n",
    "            print(\"Stopping after epoch \" + str(epoch) + \" due to early stopping criteria.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "\n",
    "all_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomRotation(90),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(\"Set up data transforms.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data configuration, like data directory and transforms\n",
    "\n",
    "# data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids\\\\images\"\n",
    "data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\preprocessed-images\"\n",
    "meta_data_loc = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\book_number_mapping.csv\"\n",
    "book_data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\books-preprocessed-images\"\n",
    "\n",
    "print(data_dir)\n",
    "print(meta_data_loc)\n",
    "print(book_data_dir)\n",
    "\n",
    "data_transforms = selected_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create data sets/loaders for each book\n",
    "\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "# test books are currently arbitrarily set\n",
    "test_books = set([\"Albin\", \"Dryden\"])\n",
    "# test_books = [\"Dryden\"]\n",
    "\n",
    "# Get the list of all books in the data set\n",
    "books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                 if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((1000,1000)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "# Create a dict of datasets for each book\n",
    "book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), transform = data_transforms[t])#, transform=test_transform)\n",
    "                      for t in set_types}\n",
    "                 for b in books_in_data}\n",
    "\n",
    "# Create the test data loader, which will only have the books in test_books\n",
    "# book_data_loaders = {}\n",
    "# book_data_loaders[\"test\"] = {b : torch.utils.data.DataLoader(book_data_sets[b][\"test\"],\n",
    "#                                                         batch_size=4,\n",
    "#                                                         num_workers=4)\n",
    "#                         for b in test_books}\n",
    "\n",
    "# for each cross-validation, the \"train\" and \"val\" parts of the data_loaders dict will be modified accordingly\n",
    "\n",
    "\n",
    "\n",
    "# class_names = book_data_sets[\"Albin\"][\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "# i = 0\n",
    "# for data in book_data_loaders[\"test\"][\"Dryden\"]:\n",
    "#     print(i)\n",
    "#     inputs, classes = data\n",
    "#     if i == 10:\n",
    "#         for ind in range(len(inputs)):\n",
    "#             img = inputs[ind]\n",
    "#             nimg = img.numpy().T\n",
    "#             imgplot = plt.imshow(nimg)\n",
    "#             print(classes[ind])\n",
    "#         break\n",
    "#     else:\n",
    "#         i += 1\n",
    "\n",
    "# print(\"visualized a few images\")\n",
    "\n",
    "######\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read meta data of the images, and make book-index mapping dictionary\n",
    "\n",
    "# Assumes meta data file:\n",
    "#   - has field names in first row\n",
    "#   - the field names are in this order: [\"id\", \"book\", \"label\"]\n",
    "\n",
    "# Create a dict of dicts:\n",
    "#   - keys are based on the value in \"label\"\n",
    "#   - values are the \"inner\" dicts described below\n",
    "# Each \"inner\" dict has:\n",
    "#   - keys are the book names, specified in \"book\"\n",
    "#   - each value is a list of all the indices denoted by \"id\"\n",
    "\n",
    "# Below is commented temporarily\n",
    "# book_mapping = {}\n",
    "\n",
    "# with open(meta_data_loc, mode='r') as infile:\n",
    "#     reader = csv.DictReader(infile)\n",
    "#     for row in reader:\n",
    "#         if row[\"label\"] not in book_mapping:\n",
    "#             book_mapping[row[\"label\"]] = {}\n",
    "#         bm = book_mapping[row[\"label\"]]\n",
    "        \n",
    "#         if row[\"book\"] not in bm:\n",
    "#             bm[row[\"book\"]] = []\n",
    "#         bm[row[\"book\"]].append(int(row[\"id\"]))\n",
    "\n",
    "# count = 0\n",
    "# for l in book_mapping:\n",
    "#     for b in book_mapping[l]:\n",
    "#         count += len(book_mapping[l][b])\n",
    "\n",
    "# print(\"read meta data for \" + str(count) + \" images from \" + str(meta_data_loc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def valid_imshow_data(data):\n",
    "    data = np.asarray(data)\n",
    "    if data.ndim == 2:\n",
    "        return True\n",
    "    elif data.ndim == 3:\n",
    "        if 3 <= data.shape[2] <= 4:\n",
    "            return True\n",
    "        else:\n",
    "            print('The \"data\" has 3 dimensions but the last dimension '\n",
    "                  'must have a length of 3 (RGB) or 4 (RGBA), not \"{}\".'\n",
    "                  ''.format(data.shape[2]))\n",
    "            return False\n",
    "    else:\n",
    "        print('To visualize an image the data must be 2 dimensional or '\n",
    "              '3 dimensional, not \"{}\".'\n",
    "              ''.format(data.ndim))\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# set_types = ['train', 'val', 'test']\n",
    "# # test books are currently arbitrarily set\n",
    "# # test_books = [\"Albin\", \"Dryden\"]\n",
    "# test_books = [\"Dryden\"]\n",
    "\n",
    "\n",
    "# test_transform = transforms.Compose([\n",
    "#     transforms.Resize((1000,1000)),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "# # Need to split data sets into training and testing sets\n",
    "# data_sets = {t : datasets.ImageFolder(data_dir, transform = test_transform)#, transform=data_transforms[t])\n",
    "#              for t in set_types}\n",
    "\n",
    "\n",
    "# data_samplers = {}\n",
    "# data_samplers[\"test\"] = sum([bm[b] for bm in book_mapping.values() for b in bm if b in test_books], [])\n",
    "# data_samplers[\"train\"] = []\n",
    "# data_samplers[\"val\"] = []\n",
    "\n",
    "\n",
    "# dataloaders = {t : torch.utils.data.DataLoader(data_sets[t],\n",
    "#                                               sampler=data_samplers[t],\n",
    "#                                               batch_size=4,\n",
    "#                                               num_workers=4)\n",
    "#               for t in set_types}\n",
    "\n",
    "# class_names = data_sets[\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# print(sorted(data_samplers[\"test\"]))\n",
    "\n",
    "# # inputs, classes = next(iter(dataloaders[\"test\"]))\n",
    "# i = 0\n",
    "# for inputs, classes in iter(dataloaders[\"test\"]):\n",
    "#     print(i)\n",
    "#     if i == 0:\n",
    "#         for img in inputs:\n",
    "#             nimg = img.numpy().T\n",
    "#             imgplot = plt.imshow(nimg)\n",
    "#         break\n",
    "#     else:\n",
    "#         i += 1\n",
    "\n",
    "# # inputs, classes = next(iter(dataloaders[\"test\"]))\n",
    "# # out = torchvision.utils.make_grid(inputs)\n",
    "# # imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "\n",
    "# # print(data_samplers)\n",
    "\n",
    "# # Get a batch of training data\n",
    "# # inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # # Make a grid from batch\n",
    "# # out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# # imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# print(\"visualized a few images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# set_types = ['train', 'val', 'test']\n",
    "\n",
    "# # Need to split data sets into training and testing sets\n",
    "# data_sets = {t : datasets.ImageFolder(data_dir, transform=data_transforms[t])\n",
    "#              for t in set_types}\n",
    "\n",
    "# # Need to split data into training set, validation set, and testing set\n",
    "# train_size = 0.65     # 65% of all data is training\n",
    "# val_size = 0.15       # 15% of all data is validation\n",
    "# test_size = (1 - train_size - val_size)    # Remaining data (20%) is testing\n",
    "\n",
    "# num_images = len(data_sets[\"train\"]) # length of both sets should be the same\n",
    "# all_ind = list(range(num_images))\n",
    "# random_seed = 11\n",
    "# np.random.seed(random_seed)\n",
    "# np.random.shuffle(all_ind)\n",
    "\n",
    "# train_split = int(num_images * train_size)\n",
    "# val_split = int(num_images * val_size)\n",
    "# test_split = int(num_images * test_size)\n",
    "\n",
    "# data_samplers = {}\n",
    "# data_samplers[\"train\"] = all_ind[:train_split]\n",
    "# data_samplers[\"val\"] = all_ind[train_split : train_split+val_split]\n",
    "# data_samplers[\"test\"] = all_ind[train_split+val_split:]\n",
    "\n",
    "# dataloaders = {t : torch.utils.data.DataLoader(data_sets[t],\n",
    "#                                               sampler=data_samplers[t],\n",
    "#                                               batch_size=4,\n",
    "#                                               num_workers=4)\n",
    "#               for t in set_types}\n",
    "\n",
    "# dataset_sizes = {t : len(data_samplers[t]) for t in set_types}\n",
    "\n",
    "# class_names = data_sets[\"train\"].classes\n",
    "# use_gpu = torch.cuda.is_available()\n",
    "\n",
    "# print(\"data loaded from \" + str(data_dir))\n",
    "# print(\"read classes: \" + str(class_names))\n",
    "# if use_gpu:\n",
    "#     print(\"use_gpu is true\")\n",
    "# else:\n",
    "#     print(\"use_gpu is false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Following is commented out temporarily\n",
    "\n",
    "# # Get a batch of training data\n",
    "# inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out, title=[class_names[x] for x in classes])\n",
    "\n",
    "# print(\"visualized a few images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_architecture():\n",
    "    print(\"Transferring resnet18 and retraining with annotations dataset.\")\n",
    "\n",
    "    model_resnet18 = models.resnet18(pretrained=True)\n",
    "    num_params = sum(1 for i in model_resnet18.parameters())\n",
    "\n",
    "    # There are 10 layers (model_ft.children()) in resnet18\n",
    "    # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "    max_layer = 5\n",
    "    curr_layer = 1\n",
    "    last_layer = None\n",
    "    for child in model_resnet18.children():\n",
    "        if curr_layer <= max_layer:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            last_layer = child\n",
    "            curr_layer = curr_layer + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # for child in model.children():\n",
    "    #     print(\"\")\n",
    "    #     print(child)\n",
    "\n",
    "    # Replace the final fully connected layer to perform binary classification\n",
    "    num_ftrs = model_resnet18.fc.in_features\n",
    "    model_resnet18.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    if use_gpu:\n",
    "        model_resnet18 = model_resnet18.cuda()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p: p.requires_grad, model_resnet18.parameters())), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Create LR scheduler that decays LR by a factor of 0.1 for every 7 epochs (this is from tutorial, might need tweaking)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    return model_resnet18\n",
    "\n",
    "print(\"Defined function to build model architecture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"perform training\")\n",
    "\n",
    "# Validation scores for determining the best number of epochs\n",
    "#    - The keys are the book names, and point to dicts that are indexed by epoch number\n",
    "all_epoch_scores = {\"train\" : {}, \"val\" : {}}\n",
    "\n",
    "num_training_epochs = 50\n",
    "\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # setup this cross val's data loaders\n",
    "    train_books = books_in_data - set([val_book])\n",
    "    \n",
    "    book_data_loaders = {}\n",
    "    book_data_loaders = {\"test\" :\n",
    "                         {b : torch.utils.data.DataLoader(book_data_sets[b][\"test\"],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for b in test_books},\n",
    "                         \"train\" :\n",
    "                         {b : torch.utils.data.DataLoader(book_data_sets[b][\"train\"],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for b in train_books},\n",
    "                         \"val\" :\n",
    "                         {b : torch.utils.data.DataLoader(book_data_sets[b][\"val\"],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for b in [val_book]}\n",
    "                        }\n",
    "    \n",
    "    # can customize this later to transfer from different models, freezing different numbers of layers, etc.\n",
    "    model_architecture = create_model_architecture()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p:\n",
    "                                      p.requires_grad, model_architecture.parameters())),\n",
    "                          lr=0.001, momentum=0.9)\n",
    "\n",
    "    # Create LR scheduler that decays LR by a factor of 0.1 for every 7 epochs (this is from tutorial, might need tweaking)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "    \n",
    "    earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "    \n",
    "    print(book_data_loaders.keys())\n",
    "\n",
    "    trained_model_weights, epoch_scores = train_model(model_architecture,\n",
    "                                                      criterion,\n",
    "                                                      optimizer,\n",
    "                                                      exp_lr_scheduler,\n",
    "                                                      book_data_loaders,\n",
    "                                                      num_epochs=num_training_epochs,\n",
    "                                                      early_stopping = earlyStoppingCriteria)\n",
    "    for t in epoch_scores:\n",
    "        all_epoch_scores[t][val_book] = epoch_scores[t]\n",
    "    \n",
    "# Average scores over the books for each epoch\n",
    "ave_val_scores = {t : {} for t in all_epoch_scores}\n",
    "for t in all_epoch_scores:\n",
    "    for epoch in range(num_training_epochs):\n",
    "        count = 0\n",
    "        cum = 0\n",
    "        for book in all_epoch_scores[t]:\n",
    "            if epoch in all_epoch_scores[t][book]:\n",
    "                cum += all_epoch_scores[t][book][epoch]\n",
    "                count += 1\n",
    "        ave_val_scores[t][epoch] = (cum/count)\n",
    "\n",
    "print(ave_val_scores)\n",
    "\n",
    "best_epochs = {t : max(ave_val_scores[t], key=ave_val_scores.get) for t in ave_val_scores}\n",
    "print(best_epochs)\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_path = \"resnet18_half_frozen_5epochs_transfer-state.pt\"\n",
    "torch.save(model_resnet18.state_dict(), weights_path)\n",
    "print(\"saved model state (weights) to \" + weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['test']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_resnet18)\n",
    "print(\"visualizing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"running on testing dataset\")\n",
    "model_resnet18.train(False)  # Set model to evaluate mode\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over data.\n",
    "for data in dataloaders[\"test\"]:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    outputs = model_resnet18(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # backward + optimize only if in training phase\n",
    "#     if phase == 'train':\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / dataset_sizes[\"test\"]\n",
    "epoch_acc = running_corrects / dataset_sizes[\"test\"]\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "    \"test\", epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
