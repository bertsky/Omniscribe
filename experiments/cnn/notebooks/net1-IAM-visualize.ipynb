{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "net1_data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor()\n",
    "        \n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    \n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(64),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../../'))\n",
    "from detection.lib.model.ImageROI import ImageROI\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# book_data_dir = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images'\n",
    "# img_loc = book_data_dir + '/Dryden/positive/307.png'\n",
    "# Testing with image of dimensions 1000x1381\n",
    "\n",
    "#img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Allestree/positive/1307.png'\n",
    "\n",
    "#img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Confucius/positive/263.png'\n",
    "img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Albin/positive/164.png'\n",
    "#img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Voltaire/positive/11.png'\n",
    "#img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Defoe/positive/103.png'\n",
    "#img_loc = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images/Defoe/negative/1981.png'\n",
    "\n",
    "\n",
    "pilimg = Image.open(img_loc)\n",
    "w, h = pilimg.size\n",
    "imgrois = [ImageROI(0, 0, w, h)]\n",
    "imgarr = np.asarray(pilimg)\n",
    "\n",
    "from model_utils import create_model_architecture\n",
    "from bounding_box_classification import get_pos_rois\n",
    "# net1 = Net1()\n",
    "net1 = create_model_architecture('net1', use_gpu = False)\n",
    "# net1.load_state_dict(torch.load('models/net1-IAM-1.pt'))\n",
    "net1.load_state_dict(torch.load('models/net1-aa_64-500.pt'))\n",
    "\n",
    "\n",
    "# Modify this as necessary\n",
    "# imgrois = [ImageROI(0, 0, 1000, 700), ImageROI(0, 750, 1000, 500)]\n",
    "net1.train(False)\n",
    "with torch.set_grad_enabled(False):\n",
    "    pos_rois = get_pos_rois(net1,\n",
    "                            (img_loc, imgrois),\n",
    "                            model_transform = net1_data_transforms['test'],\n",
    "                            model_input_size = (64,64),\n",
    "                            stride=(32, 32))\n",
    "\n",
    "# for rois in pos_rois:\n",
    "#     for roi in rois:\n",
    "#         print(roi.toString())\n",
    "        \n",
    "all_rois = [roi for rois in pos_rois for roi in rois]\n",
    "confs = [roi.confidence for roi in all_rois]\n",
    "\n",
    "\n",
    "\n",
    "#print(confs)\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# n, bins, patches = plt.hist(confs,908)\n",
    "# plt.show()\n",
    "    \n",
    "# print(bins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(len(all_rois))\n",
    "threshold = -7.58074643e-04\n",
    "thresholded_rois = [roi for roi in all_rois if roi.confidence >= threshold]\n",
    "print(len(thresholded_rois))\n",
    "print(thresholded_rois[0].toString())\n",
    "\n",
    "\n",
    "\n",
    "from detection.lib.utils.Utils import plt_img\n",
    "plt_img(imgarr, rois=[(thresholded_rois, 'r')])#, roi_level_set=roi_level_set)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print('number of pos ROIs found', len(all_rois))\n",
    "\n",
    "# from detection.lib.ImgProcessor import ImgProcessor\n",
    "#from detection.lib.utils.Utils import plt_img\n",
    "# plot the rois and plot only certain information according to the set\n",
    "# imgProcessor = ImgProcessor()\n",
    "# img = imgProcessor.loadImage(inFilename=img_loc)\n",
    "# roi_level_set = set([1])\n",
    "#plt_img(imgarr, rois=[(all_rois, 'r')])#, roi_level_set=roi_level_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"done plotting regions of interest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
