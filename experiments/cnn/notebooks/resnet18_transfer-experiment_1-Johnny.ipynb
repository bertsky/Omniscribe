{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 with resnet18 transfer learning\n",
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import torchnet as tnt\n",
    "from utils import *\n",
    "from classes import *\n",
    "\n",
    "# plt setup and the gpu setup\n",
    "plt.ion()\n",
    "use_gpu = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1. define data transform\n",
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "# ========================================\n",
    "\n",
    "print(\"Set up data transforms.\")\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # should not do random transformation in val or test set\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224,224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "data_transforms = selected_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2. define and load data\n",
    "# ========================================\n",
    "print(\"Create dataset and dataloader\")\n",
    "\n",
    "# data location\n",
    "book_data_dir = \"/home/kcho/clab_data/\"\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "# test books are currently arbitrarily set\n",
    "test_books = set([\"Albin\", \"Dryden\"])\n",
    "\n",
    "# Get the list of all books in the data set\n",
    "books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                 if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "\n",
    "# Create a dict of datasets for each book\n",
    "book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), \n",
    "                                                transform = data_transforms[t])#, transform=test_transform)\n",
    "                      for t in set_types}\n",
    "                 for b in books_in_data}\n",
    "\n",
    "# create a dict of dataloaders, book_data_loaders['Albin']['train']\n",
    "book_data_loaders = {b : {t : torch.utils.data.DataLoader(book_data_sets[b][t],\n",
    "                                                          batch_size=4,\n",
    "                                                          num_workers=4)\n",
    "                          for t in set_types}\n",
    "                     for b in books_in_data}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named utee",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-093aeb72fef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# ========================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutee\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_model_architecture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named utee"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 3. define model structure\n",
    "# ========================================\n",
    "\n",
    "from utee import selector\n",
    "\n",
    "def create_model_architecture():\n",
    "    print(\"Transferring resnet18 and retraining with annotations dataset.\")\n",
    "\n",
    "    model_resnet18 = models.resnet18(pretrained=True)\n",
    "    num_params = sum(1 for i in model_resnet18.parameters())\n",
    "\n",
    "    # There are 10 layers (model_ft.children()) in resnet18\n",
    "    # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "    max_layer = 5\n",
    "    curr_layer = 1\n",
    "    last_layer = None\n",
    "    for child in model_resnet18.children():\n",
    "        if curr_layer <= max_layer:\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "            last_layer = child\n",
    "            curr_layer = curr_layer + 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Replace the final fully connected layer to perform binary classification\n",
    "    num_ftrs = model_resnet18.fc.in_features\n",
    "    model_resnet18.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "    # return\n",
    "    if use_gpu:\n",
    "        return model_resnet18.cuda()\n",
    "    else:\n",
    "        return model_resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, data_loaders, num_epochs=25, early_stopping = None):\n",
    "    \n",
    "    # parameter instantiation\n",
    "    time_begin = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0   \n",
    "    val_acc_loss = 0.0\n",
    "    epoch_acc_dict = {\"train\": {}, \"val\" : {}}\n",
    "\n",
    "    # stop the training, validation, and test loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            # looping parameters\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            count = 0\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "\n",
    "            # Iterate over each book\n",
    "            for book in data_loaders[phase]:\n",
    "                \n",
    "                # Iterate over data.\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                    count += len(inputs)\n",
    "                    \n",
    "                    # wrap them in Variable\n",
    "                    if use_gpu:\n",
    "                        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                    # Use adam instead of sgd so you don't need to worry about the learning rate changes\n",
    "                    optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / count\n",
    "            epoch_acc = running_corrects / count\n",
    "        \n",
    "            epoch_acc_dict[phase][epoch] = epoch_acc\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(confusion_matrix.conf)\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # store most recent val_loss\n",
    "            if phase == 'val':\n",
    "                val_acc_loss = (1.0 - epoch_acc)\n",
    "                print(\"val_acc_loss = \" + str(val_acc_loss))\n",
    "\n",
    "                        # save each epoch's model\n",
    "#             weights_path = \"resnet18_half_frozen_\" + str(epoch + 1) + \"epochs_transfer-state.pt\"\n",
    "#             torch.save(model_resnet18.state_dict(), weights_path)\n",
    "#             print(\"saved epoch \" + str(epoch + 1) + \" model state (weights) to \" + weights_path)\n",
    "#             print(\"ran epoch \" + str(epoch + 1))\n",
    "        \n",
    "        \n",
    "        # Extra spacing\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Include early stopping criteria check at end of epoch\n",
    "        if (early_stopping is not None) and early_stopping.checkStoppingCriteria(val_acc_loss):\n",
    "            print(\"Stopping after epoch \" + str(epoch) + \" due to early stopping criteria.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    # calculate the training time\n",
    "    time_elapsed = time.time() - begin\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined function to build model architecture.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perform training\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_books' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8e2a4d3420b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mcross_val_loaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcross_val_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbook_data_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_books\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval_book\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbooks_in_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_books' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"perform training\")\n",
    "\n",
    "# Validation scores for determining the best number of epochs\n",
    "#    - The keys are the book names, and point to dicts that are indexed by epoch number\n",
    "all_epoch_scores = {\"train\" : {}, \"val\" : {}}\n",
    "\n",
    "num_training_epochs = 50\n",
    "\n",
    "cross_val_loaders = {}\n",
    "cross_val_loaders[\"test\"] = {b : book_data_loaders[b][\"test\"] for b in test_books}\n",
    "\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # setup this cross val's data loaders\n",
    "    train_books = books_in_data - set([val_book])\n",
    "    \n",
    "    cross_val_loaders[\"train\"] = {b : book_data_loaders[b][\"train\"] for b in train_books}\n",
    "    cross_val_loaders[\"val\"] = {b : book_data_loaders[b][\"val\"] for b in [val_book]}\n",
    "    \n",
    "    # can customize this later to transfer from different models, freezing different numbers of layers, etc.\n",
    "    model_architecture = create_model_architecture()\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Need to create slightly custom optimizer since half of the layers are frozen\n",
    "    optimizer = optim.SGD(list(filter(lambda p:\n",
    "                                      p.requires_grad, model_architecture.parameters())),\n",
    "                          lr=0.001, momentum=0.9)\n",
    "   \n",
    "    earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "    \n",
    "    print(\"Cross val loader:\", cross_val_loaders.keys())\n",
    "\n",
    "    trained_model_weights, epoch_scores = train_model(model_architecture,\n",
    "                                                      criterion,\n",
    "                                                      optimizer,\n",
    "                                                      cross_val_loaders,\n",
    "                                                      num_epochs=num_training_epochs,\n",
    "                                                      early_stopping = earlyStoppingCriteria)\n",
    "    # Attempt to resolve memory issue\n",
    "    gc.collect()\n",
    "    # testing for memory management\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    for t in epoch_scores:\n",
    "        all_epoch_scores[t][val_book] = epoch_scores[t]\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average scores over the books for each epoch\n",
    "ave_val_scores = {t : {} for t in all_epoch_scores}\n",
    "for t in all_epoch_scores:\n",
    "    for epoch in range(num_training_epochs):\n",
    "        count = 0\n",
    "        cum = 0\n",
    "        for book in all_epoch_scores[t]:\n",
    "            if epoch in all_epoch_scores[t][book]:\n",
    "                cum += all_epoch_scores[t][book][epoch]\n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            ave_val_scores[t][epoch] = (cum/count)\n",
    "\n",
    "print(ave_val_scores)\n",
    "\n",
    "best_epochs = {t : max(ave_val_scores[t], key=ave_val_scores[t].get) for t in ave_val_scores}\n",
    "print(best_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"resnet18_half_frozen_5epochs_transfer-state.pt\"\n",
    "torch.save(model_resnet18.state_dict(), weights_path)\n",
    "print(\"saved model state (weights) to \" + weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['test']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_resnet18)\n",
    "print(\"visualizing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"running on testing dataset\")\n",
    "model_resnet18.train(False)  # Set model to evaluate mode\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over data.\n",
    "for data in dataloaders[\"test\"]:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    outputs = model_resnet18(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # backward + optimize only if in training phase\n",
    "#     if phase == 'train':\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / dataset_sizes[\"test\"]\n",
    "epoch_acc = running_corrects / dataset_sizes[\"test\"]\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "    \"test\", epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
