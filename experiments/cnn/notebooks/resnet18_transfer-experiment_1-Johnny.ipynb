{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 with resnet18 transfer learning\n",
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import torchnet as tnt\n",
    "from utils import *\n",
    "from classes import *\n",
    "from tqdm import tqdm_notebook # for-loop progress bar in notebook\n",
    "\n",
    "# plt setup and the gpu setup\n",
    "plt.ion()\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "# load memory profiler\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom Dataset object\n",
    "# A unique object will need to be created for each book.\n",
    "# It will take in all the transforms = a dict with key, val where:\n",
    "#   key=phase, val=tranform to use\n",
    "\n",
    "# To create custom Dataset\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.datasets.folder import find_classes, make_dataset, IMG_EXTENSIONS, default_loader\n",
    "\n",
    "class BookDatasetContainer:\n",
    "    pass\n",
    "\n",
    "class BookDataset:\n",
    "    pass\n",
    "\n",
    "class BookDatasetContainer:\n",
    "    __samples = None\n",
    "    __images = []\n",
    "    \n",
    "    def __init__(self, root, loader=default_loader):\n",
    "        # Read in all images\n",
    "        classes, class_to_idx = find_classes(root)\n",
    "        self.__samples = make_dataset(root, class_to_idx, IMG_EXTENSIONS)\n",
    "        \n",
    "#         print(root)\n",
    "#         %memit\n",
    "        self.__images = [(loader(path), target) for path, target in self.__samples]\n",
    "#         %memit\n",
    "#         print()\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.__samples)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.__images[index]\n",
    "    \n",
    "    # Method of BookDatasetContainer to create a BookDataset with a specific transform\n",
    "    def create_dataset(self, transform):\n",
    "        return BookDataset(self, transform)\n",
    "    \n",
    "# BookDataset class uses the BookDatasetContainer's images, adding transform functionality\n",
    "class BookDataset(Dataset):\n",
    "    __dataset_container = None\n",
    "    __transform = None\n",
    "\n",
    "    def __init__(self, dataset_container, transform):\n",
    "#         %memit\n",
    "        self.__dataset_container = dataset_container\n",
    "        self.__transform = transform\n",
    "#         %memit\n",
    "#         print()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample, target = self.__dataset_container[index]\n",
    "        if (self.__transform is not None):\n",
    "            sample = self.__transform(sample)\n",
    "        return sample, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.__dataset_container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1. define data transform\n",
    "#\n",
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "# ========================================\n",
    "\n",
    "# Need the __name__ check to make multiprocessing work on Windows for some reason\n",
    "if __name__ == '__main__':\n",
    "    print(\"Set up data transforms.\")\n",
    "    img_input_size = 56\n",
    "\n",
    "    selected_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            # RandomRotation does not seem to be working on Windows right now\n",
    "            transforms.RandomRotation(45),\n",
    "\n",
    "            transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "            transforms.RandomGrayscale(p=0.1),\n",
    "\n",
    "            transforms.Grayscale(), # not sure why the current input is not grayscale, do grayscale conversion\n",
    "            transforms.Resize((img_input_size,img_input_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "        # should not do random transformation in val or test set\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((img_input_size,img_input_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Grayscale(),\n",
    "            transforms.Resize((img_input_size,img_input_size)),\n",
    "            transforms.ToTensor(),\n",
    "        ]),\n",
    "    }\n",
    "    data_transforms = selected_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create dataset and dataloader\n",
      "Using custom dataset.\n",
      "now starting data loaders\n",
      "data loading complete\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2. define and load data\n",
    "#\n",
    "# TODO: Suggest not using data loader to load the images. Because it is slow. Every epochs, you load the \n",
    "# image data again to the RAM and then from RAM to GPU RAM. That takes a lot of time. Since the images are \n",
    "# <1GB, you can preload the images into RAM first, that will make you training way faster. \n",
    "\n",
    "# Note: you only has about 200MB of images, it's small. It should not take much time to load!\n",
    "# ========================================\n",
    "\n",
    "# This flag is on if the custom dataset BookDataset should be used\n",
    "use_custom_dataset = True\n",
    "\n",
    "# Need the __name__ check to make multiprocessing work on Windows for some reason\n",
    "# Unfortunately, still getting broken pipe error on Windows with custom dataset.\n",
    "if __name__ == '__main__':\n",
    "    print(\"Create dataset and dataloader\")\n",
    "\n",
    "    # hyperparameter\n",
    "    # batch_size = 50 # larger batch size is better so you can load more data into gpu and train faster\n",
    "    batch_size = 50 # larger batch size is better so you can load more data into gpu and train faster\n",
    "\n",
    "    # data location\n",
    "#     book_data_dir = \"/home/kcho/clab_data/\"\n",
    "#     book_data_dir = \"C:\\\\Users\\\\rahul\\\\Documents\\\\work\\\\BuildUCLA\\\\data\\\\printed_with_ids_harsh_filter\\\\books-preprocessed-images\"\n",
    "    book_data_dir = '/home/rahul/data/printed_with_ids_harsh_filter/books-preprocessed-images'\n",
    "    set_types = ['train', 'val', 'test']\n",
    "\n",
    "    # test books are currently arbitrarily set\n",
    "    test_books = set([\"Albin\", \"Dryden\"])\n",
    "\n",
    "    # Get the list of all books in the data set\n",
    "    books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                     if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "    \n",
    "    if use_custom_dataset:\n",
    "        print('Using custom dataset.')\n",
    "        \n",
    "#         %memit\n",
    "        book_data_sets = {b : BookDatasetContainer(os.path.join(book_data_dir, b))\n",
    "                          for b in books_in_data}\n",
    "#         %memit\n",
    "        \n",
    "#         print()\n",
    "        print(\"now starting data loaders\")\n",
    "#         %memit\n",
    "#         print()\n",
    "        book_data_loaders = {b : {t : torch.utils.data.DataLoader(\n",
    "                                  book_data_sets[b].create_dataset(data_transforms[t]),\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True, # make sure you shuffle the data\n",
    "            num_workers=4)\n",
    "                                  for t in set_types}\n",
    "                             for b in books_in_data}\n",
    "#         %memit\n",
    "#         print()\n",
    "        \n",
    "    else:\n",
    "        print('Using default ImageFolder dataset.')\n",
    "#         tt = transforms.Compose([\n",
    "#             transforms.Grayscale(),\n",
    "#             transforms.Resize((img_input_size,img_input_size)),\n",
    "#             transforms.ToTensor(),\n",
    "#         ])\n",
    "#         book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), \n",
    "#                                                         transform = tt)#, transform=test_transform)\n",
    "#                               for t in set_types}\n",
    "#                          for b in books_in_data}\n",
    "        # Create a dict of datasets for each book\n",
    "        book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), \n",
    "                                                        transform = data_transforms[t])#, transform=test_transform)\n",
    "                              for t in set_types}\n",
    "                         for b in books_in_data}\n",
    "\n",
    "\n",
    "        # create a dict of dataloaders, book_data_loaders['Albin']['train']\n",
    "        book_data_loaders = {b : {t : torch.utils.data.DataLoader(book_data_sets[b][t],\n",
    "                                                                  batch_size=batch_size,\n",
    "                                                                  shuffle=True, # make sure you shuffle the data\n",
    "                                                                  num_workers=4)\n",
    "                                  for t in set_types}\n",
    "                             for b in books_in_data}\n",
    "    \n",
    "\n",
    "    print(\"data loading complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 56, 56)\n",
      "(56, 56)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Check your data to make sure it is correct\n",
    "# ========================================\n",
    "\n",
    "# Need the __name__ check to make multiprocessing work on Windows for some reason\n",
    "if __name__ == '__main__':\n",
    "    for data in book_data_loaders['Defoe']['train']:\n",
    "        # get the inputs\n",
    "        inputs, labels = data\n",
    "\n",
    "        # plot to verify the input is correct\n",
    "        input1 = inputs[0].numpy()\n",
    "        print(input1.shape)\n",
    "        input1 = np.swapaxes(input1,0,2).squeeze()\n",
    "        print(input1.shape)\n",
    "        plt.imshow(input1,cmap='gray')\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained mnist model\n",
      "Building and initializing mnist parameters\n",
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=784, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Dropout(p=0.2)\n",
      "    (5): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.2)\n",
      "    (fc): Linear(in_features=256, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 3. define model structure\n",
    "#\n",
    "# TODO: Stick with MNIST for now because it is smaller network and faster to train. Note that you lose\n",
    "# a lot of inofmration when yo udownsample the image to 28x28. THat's okay, you optimize other parts first, e.g.,\n",
    "# data loading before you optimize your model.\n",
    "# ========================================\n",
    "from lib.playground.utee import selector\n",
    "from lib.playground.mnist import model\n",
    "import os\n",
    "\n",
    "def create_model_architecture(model_type='mnist'):\n",
    "    \"\"\"\n",
    "    params model_type: the type of model, for now, support mnist and resnet18    \n",
    "    \"\"\"\n",
    "    if model_type == 'mnist':\n",
    "        print('using pretrained mnist model')\n",
    "        \n",
    "        # load the model from the playground library\n",
    "        model_annotation, ds_fetcher, is_imagenet = selector.select('mnist')\n",
    "        \n",
    "        # remove last layer\n",
    "        removed = list(model_annotation.model.children())[:-1]\n",
    "        \n",
    "        # add a front layer to account for new input\n",
    "        # IMPORTANT, we need to update the self.input_dims of the MLP class\n",
    "        removed = [nn.Linear(img_input_size*img_input_size,28*28), nn.ReLU()] + removed\n",
    "        \n",
    "        # formulate the layers\n",
    "        model_annotation.model=torch.nn.Sequential(*removed)\n",
    "        \n",
    "        # add the new fc layer\n",
    "        model_annotation.model.fc = torch.nn.Linear(256,2).cuda()\n",
    "        \n",
    "        # update the self.input_dims of the network\n",
    "        model_annotation.input_dims = img_input_size * img_input_size                \n",
    "\n",
    "    elif model_type == 'resnet18':    \n",
    "        print(\"Transferring resnet18 and retraining with annotations dataset.\")    \n",
    "        model_annotation = models.resnet18(pretrained=True)\n",
    "        num_params = sum(1 for i in model_annotation.parameters())\n",
    "\n",
    "        # There are 10 layers (model_ft.children()) in resnet18\n",
    "        # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "        max_layer = 5\n",
    "        curr_layer = 1\n",
    "        last_layer = None\n",
    "        for child in model_annotation.children():\n",
    "            if curr_layer <= max_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "                last_layer = child\n",
    "                curr_layer = curr_layer + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Replace the final fully connected layer to perform binary classification\n",
    "        num_ftrs = model_annotation.fc.in_features\n",
    "        model_annotation.fc = nn.Linear(num_ftrs, 2)\n",
    "        \n",
    "\n",
    "    # return\n",
    "    if use_gpu:\n",
    "        return model_annotation.cuda()\n",
    "    else:\n",
    "        return model_annotation.cpu()\n",
    "\n",
    "    \n",
    "net = create_model_architecture(model_type='mnist')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 4. define the training process.\n",
    "#\n",
    "# TODO: The basic process of train and validation is defined. Please implement the overall average validation \n",
    "# confusion matrix, meaning for each validation (after running all epochs), get the confusion matrix for that book,\n",
    "# repeat this for the rest of 10 books. THen get the overall performance. Also, implement the early-stop as you\n",
    "# originally has in your code. :) I removed them here for clarity. You can add them back. \n",
    "# ========================================\n",
    "\n",
    "def train(model, criterion, optimizer, data_loaders, num_epochs=25, early_stopping = None):\n",
    "    since = time.time()\n",
    "\n",
    "    # stop the training, validation, and test loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # looping parameters\n",
    "        running_loss = 0.0\n",
    "        confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "        \n",
    "        # loop through train and val phase in each epoch\n",
    "        for phase in ['train', 'val']:\n",
    "            # check train or val\n",
    "            if phase == 'train':                \n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over each book\n",
    "            running_loss = 0.0\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "#             for book in tqdm_notebook(data_loaders[phase]): \n",
    "            for book in data_loaders[phase]:\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs;  wrap them in Variable and make them into gpu or not\n",
    "                    inputs, labels = data # input size: [5, 1, 28, 28] ; keep the dummy color channel:1\n",
    "                    if use_gpu:\n",
    "                        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # back\n",
    "                    if phase == 'train': \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0) \n",
    "            \n",
    "            # report evaluation\n",
    "            print('Phase:%s' %phase)\n",
    "            print('Confusion matrix:\\n', confusion_matrix.conf)\n",
    "            print('validation loss', running_loss)\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained mnist model\n",
      "Building and initializing mnist parameters\n",
      "Epoch 0/0\n",
      "----------\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:60: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([28, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([21, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([17, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([28, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([24, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([36, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([10, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([10, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([48, 1, 56, 56])\n",
      "3136\n",
      "Phase:train\n",
      "Confusion matrix:\n",
      " [[ 198  543]\n",
      " [ 291 1690]]\n",
      "validation loss tensor(1671.7223, device='cuda:0')\n",
      "torch.Size([50, 1, 56, 56])\n",
      "3136\n",
      "torch.Size([46, 1, 56, 56])\n",
      "3136\n",
      "Phase:val\n",
      "Confusion matrix:\n",
      " [[ 0 20]\n",
      " [ 0 76]]\n",
      "validation loss tensor(64.4888, device='cuda:0')\n",
      "Training complete in 0m 33s\n",
      "training complete\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 5. execute the train process\n",
    "# ========================================\n",
    "\n",
    "# get the model\n",
    "model = create_model_architecture()\n",
    "\n",
    "# train parameters\n",
    "# num_training_epochs = 5\n",
    "num_training_epochs = 1\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "\n",
    "# dataloader parameters\n",
    "cross_val_loaders = {}\n",
    "\n",
    "# leave-one-book-out cross validation\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # define the train and validation loaders\n",
    "    train_books = books_in_data - set([val_book])    \n",
    "    cross_val_loaders[\"train\"] = {b : book_data_loaders[b][\"train\"] for b in train_books}\n",
    "    cross_val_loaders[\"val\"] = {b : book_data_loaders[b][\"val\"] for b in [val_book]}\n",
    "\n",
    "    # train\n",
    "    trained_model = train(model,\n",
    "                          criterion,\n",
    "                          optimizer,\n",
    "                          cross_val_loaders,\n",
    "                          num_epochs=num_training_epochs)\n",
    "    \n",
    "    # remove the break as needed\n",
    "    break\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
