{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 with resnet18 transfer learning\n",
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import torchnet as tnt\n",
    "from utils import *\n",
    "from classes import *\n",
    "\n",
    "# plt setup and the gpu setup\n",
    "plt.ion()\n",
    "use_gpu = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1. define data transform\n",
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "# ========================================\n",
    "\n",
    "print(\"Set up data transforms.\")\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Grayscale(), # not sure why the current input is not grayscale, do grayscale conversion\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # should not do random transformation in val or test set\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "data_transforms = selected_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create dataset and dataloader\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2. define and load data\n",
    "# ========================================\n",
    "print(\"Create dataset and dataloader\")\n",
    "\n",
    "# data location\n",
    "book_data_dir = \"/home/kcho/clab_data/\"\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "# test books are currently arbitrarily set\n",
    "test_books = set([\"Albin\", \"Dryden\"])\n",
    "\n",
    "# Get the list of all books in the data set\n",
    "books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                 if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "\n",
    "# Create a dict of datasets for each book\n",
    "book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), \n",
    "                                                transform = data_transforms[t])#, transform=test_transform)\n",
    "                      for t in set_types}\n",
    "                 for b in books_in_data}\n",
    "\n",
    "# create a dict of dataloaders, book_data_loaders['Albin']['train']\n",
    "book_data_loaders = {b : {t : torch.utils.data.DataLoader(book_data_sets[b][t],\n",
    "                                                          batch_size=5,\n",
    "                                                          shuffle=True, # make sure you shuffle the data\n",
    "                                                          num_workers=1)\n",
    "                          for t in set_types}\n",
    "                     for b in books_in_data}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Check your data to make sure it is correct\n",
    "# ========================================\n",
    "\n",
    "for data in book_data_loaders['Defoe']['train']:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # plot to verify the input is correct\n",
    "    input1 = inputs[0].numpy()\n",
    "    print(input1.shape)\n",
    "    input1 = np.swapaxes(input1,0,2).squeeze()\n",
    "    print(input1.shape)\n",
    "    plt.imshow(input1,cmap='gray')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 3. define model structure\n",
    "# ========================================\n",
    "from lib.playground.utee import selector\n",
    "\n",
    "def create_model_architecture(model_type='mnist'):\n",
    "    \"\"\"\n",
    "    params model_type: the type of model, for now, support mnist and resnet18    \n",
    "    \"\"\"\n",
    "    if model_type == 'mnist':\n",
    "        print('using pretrained mnist model')\n",
    "        model_annotation, ds_fetcher, is_imagenet = selector.select('mnist')\n",
    "        # remove last layer\n",
    "        removed = list(model_annotation.model.children())[:-1]\n",
    "        model_annotation.model=torch.nn.Sequential(*removed)\n",
    "        # add the new fc layer\n",
    "        model_annotation.model.fc = torch.nn.Linear(256,2).cuda()\n",
    "\n",
    "    elif model_type == 'resnet18':    \n",
    "        print(\"Transferring resnet18 and retraining with annotations dataset.\")    \n",
    "        model_annotation = models.resnet18(pretrained=True)\n",
    "        num_params = sum(1 for i in model_annotation.parameters())\n",
    "\n",
    "        # There are 10 layers (model_ft.children()) in resnet18\n",
    "        # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "        max_layer = 5\n",
    "        curr_layer = 1\n",
    "        last_layer = None\n",
    "        for child in model_annotation.children():\n",
    "            if curr_layer <= max_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "                last_layer = child\n",
    "                curr_layer = curr_layer + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Replace the final fully connected layer to perform binary classification\n",
    "        num_ftrs = model_annotation.fc.in_features\n",
    "        model_annotation.fc = nn.Linear(num_ftrs, 2)\n",
    "        \n",
    "\n",
    "    # return\n",
    "    if use_gpu:\n",
    "        return model_annotation.cuda()\n",
    "    else:\n",
    "        return model_annotation.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 4. define the training process\n",
    "# ========================================\n",
    "\n",
    "def train(model, criterion, optimizer, data_loaders, num_epochs=25, early_stopping = None):\n",
    "    # parameter instantiation\n",
    "    time_begin = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0   \n",
    "    val_acc_loss = 0.0\n",
    "    epoch_acc_dict = {\"train\": {}, \"val\" : {}}\n",
    "\n",
    "    # stop the training, validation, and test loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # looping parameters\n",
    "        running_loss = 0.0\n",
    "        confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over each book\n",
    "            for book in data_loaders[phase]:\n",
    "                \n",
    "                # Iterate over data.\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs;  wrap them in Variable and make them into gpu or not\n",
    "                    inputs, labels = data # input size: [5, 1, 28, 28] ; keep the dummy color channel:1\n",
    "                    if use_gpu:\n",
    "                        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    print(labels)\n",
    "                    fdsf\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / count\n",
    "            epoch_acc = running_corrects / count\n",
    "        \n",
    "            epoch_acc_dict[phase][epoch] = epoch_acc\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(confusion_matrix.conf)\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # store most recent val_loss\n",
    "            if phase == 'val':\n",
    "                val_acc_loss = (1.0 - epoch_acc)\n",
    "                print(\"val_acc_loss = \" + str(val_acc_loss))\n",
    "\n",
    "                        # save each epoch's model\n",
    "#             weights_path = \"resnet18_half_frozen_\" + str(epoch + 1) + \"epochs_transfer-state.pt\"\n",
    "#             torch.save(model_resnet18.state_dict(), weights_path)\n",
    "#             print(\"saved epoch \" + str(epoch + 1) + \" model state (weights) to \" + weights_path)\n",
    "#             print(\"ran epoch \" + str(epoch + 1))\n",
    "        \n",
    "        \n",
    "        # Extra spacing\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Include early stopping criteria check at end of epoch\n",
    "        if (early_stopping is not None) and early_stopping.checkStoppingCriteria(val_acc_loss):\n",
    "            print(\"Stopping after epoch \" + str(epoch) + \" due to early stopping criteria.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    # calculate the training time\n",
    "    time_elapsed = time.time() - begin\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained mnist model\n",
      "Building and initializing mnist parameters\n",
      "Epoch 0/49\n",
      "----------\n",
      "tensor([ 1,  1,  1,  0,  0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'fdsf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-146701b7d678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                 \u001b[0mcross_val_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_training_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                                 early_stopping = earlyStoppingCriteria)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# testing for memory management\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-122-7f0e69bd97b1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, data_loaders, num_epochs, early_stopping)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                     \u001b[0mfdsf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                     \u001b[0;31m# Add to confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'fdsf' is not defined"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 5. execute the train process\n",
    "# ========================================\n",
    "\n",
    "# get the model\n",
    "model = create_model_architecture()\n",
    "\n",
    "# train parameters\n",
    "num_training_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "\n",
    "# other parameters\n",
    "all_epoch_scores = {\"train\" : {}, \"val\" : {}}\n",
    "cross_val_loaders = {}\n",
    "\n",
    "\n",
    "# leave-one-book-out cross validation\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # define the train and validation loaders\n",
    "    train_books = books_in_data - set([val_book])    \n",
    "    cross_val_loaders[\"train\"] = {b : book_data_loaders[b][\"train\"] for b in train_books}\n",
    "    cross_val_loaders[\"val\"] = {b : book_data_loaders[b][\"val\"] for b in [val_book]}\n",
    "\n",
    "    # train\n",
    "    trained_model_weights, epoch_scores = train(model,\n",
    "                                                criterion,\n",
    "                                                optimizer,\n",
    "                                                cross_val_loaders,\n",
    "                                                num_epochs=num_training_epochs,\n",
    "                                                early_stopping = earlyStoppingCriteria)\n",
    "\n",
    "    # testing for memory management\n",
    "    #torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 4. define the training process\n",
    "# ========================================\n",
    "\n",
    "def train(model, criterion, optimizer, data_loaders, num_epochs=25, early_stopping = None):   \n",
    "    # parameter instantiation\n",
    "    time_begin = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0   \n",
    "    val_acc_loss = 0.0\n",
    "    epoch_acc_dict = {\"train\": {}, \"val\" : {}}\n",
    "\n",
    "    # stop the training, validation, and test loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            # looping parameters\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            count = 0\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "\n",
    "            # Iterate over each book\n",
    "            for book in data_loaders[phase]:\n",
    "                \n",
    "                # Iterate over data.\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs\n",
    "                    inputs, labels = data\n",
    "                    \n",
    "                    count += len(inputs)\n",
    "                    \n",
    "                    # wrap them in Variable\n",
    "                    if use_gpu:\n",
    "                        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "                    # Use adam instead of sgd so you don't need to worry about the learning rate changes\n",
    "                    optimizer = optim.Adam(model_resnet18.parameters(), lr=0.001)\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#             epoch_loss = running_loss / dataset_sizes[phase]\n",
    "#             epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "\n",
    "            epoch_loss = running_loss / count\n",
    "            epoch_acc = running_corrects / count\n",
    "        \n",
    "            epoch_acc_dict[phase][epoch] = epoch_acc\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # Print confusion matrix\n",
    "            print(confusion_matrix.conf)\n",
    "            print()\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            # store most recent val_loss\n",
    "            if phase == 'val':\n",
    "                val_acc_loss = (1.0 - epoch_acc)\n",
    "                print(\"val_acc_loss = \" + str(val_acc_loss))\n",
    "\n",
    "                        # save each epoch's model\n",
    "#             weights_path = \"resnet18_half_frozen_\" + str(epoch + 1) + \"epochs_transfer-state.pt\"\n",
    "#             torch.save(model_resnet18.state_dict(), weights_path)\n",
    "#             print(\"saved epoch \" + str(epoch + 1) + \" model state (weights) to \" + weights_path)\n",
    "#             print(\"ran epoch \" + str(epoch + 1))\n",
    "        \n",
    "        \n",
    "        # Extra spacing\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        # Include early stopping criteria check at end of epoch\n",
    "        if (early_stopping is not None) and early_stopping.checkStoppingCriteria(val_acc_loss):\n",
    "            print(\"Stopping after epoch \" + str(epoch) + \" due to early stopping criteria.\")\n",
    "            break\n",
    "        \n",
    "\n",
    "    # calculate the training time\n",
    "    time_elapsed = time.time() - begin\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, epoch_acc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average scores over the books for each epoch\n",
    "ave_val_scores = {t : {} for t in all_epoch_scores}\n",
    "for t in all_epoch_scores:\n",
    "    for epoch in range(num_training_epochs):\n",
    "        count = 0\n",
    "        cum = 0\n",
    "        for book in all_epoch_scores[t]:\n",
    "            if epoch in all_epoch_scores[t][book]:\n",
    "                cum += all_epoch_scores[t][book][epoch]\n",
    "                count += 1\n",
    "        if count != 0:\n",
    "            ave_val_scores[t][epoch] = (cum/count)\n",
    "\n",
    "print(ave_val_scores)\n",
    "\n",
    "best_epochs = {t : max(ave_val_scores[t], key=ave_val_scores[t].get) for t in ave_val_scores}\n",
    "print(best_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"resnet18_half_frozen_5epochs_transfer-state.pt\"\n",
    "torch.save(model_resnet18.state_dict(), weights_path)\n",
    "print(\"saved model state (weights) to \" + weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    for i, data in enumerate(dataloaders['test']):\n",
    "        inputs, labels = data\n",
    "        if use_gpu:\n",
    "            inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        else:\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "\n",
    "        for j in range(inputs.size()[0]):\n",
    "            images_so_far += 1\n",
    "            ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "            ax.axis('off')\n",
    "            ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "            imshow(inputs.cpu().data[j])\n",
    "\n",
    "            if images_so_far == num_images:\n",
    "                model.train(mode=was_training)\n",
    "                return\n",
    "    model.train(mode=was_training)\n",
    "\n",
    "visualize_model(model_resnet18)\n",
    "print(\"visualizing model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"running on testing dataset\")\n",
    "model_resnet18.train(False)  # Set model to evaluate mode\n",
    "\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Iterate over data.\n",
    "for data in dataloaders[\"test\"]:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # wrap them in Variable\n",
    "    if use_gpu:\n",
    "        inputs = Variable(inputs.cuda())\n",
    "        labels = Variable(labels.cuda())\n",
    "    else:\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "\n",
    "    # zero the parameter gradients\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "    # forward\n",
    "    outputs = model_resnet18(inputs)\n",
    "    _, preds = torch.max(outputs.data, 1)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # backward + optimize only if in training phase\n",
    "#     if phase == 'train':\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "    # statistics\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "epoch_loss = running_loss / dataset_sizes[\"test\"]\n",
    "epoch_acc = running_corrects / dataset_sizes[\"test\"]\n",
    "\n",
    "print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "    \"test\", epoch_loss, epoch_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
