{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 1 with resnet18 transfer learning\n",
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import csv\n",
    "import gc\n",
    "import torchnet as tnt\n",
    "from utils import *\n",
    "from classes import *\n",
    "from tqdm import tqdm_notebook # for-loop progress bar in notebook\n",
    "\n",
    "# plt setup and the gpu setup\n",
    "plt.ion()\n",
    "use_gpu = torch.cuda.is_available()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set up data transforms.\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 1. define data transform\n",
    "#\n",
    "# Including different forms of data augmentation\n",
    "# One will include nearly all types (excluding random crops, etc. that may remove handwriting.)\n",
    "# The other will include a selected set of augmentations\n",
    "# Keeping 'train', 'val', and 'test' transforms just in case we want to include different functionalities\n",
    "# ========================================\n",
    "\n",
    "print(\"Set up data transforms.\")\n",
    "\n",
    "selected_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(45),\n",
    "        \n",
    "        transforms.ColorJitter(brightness=0, contrast=0, saturation=0, hue=0.5),\n",
    "        transforms.RandomGrayscale(p=0.1),\n",
    "        \n",
    "        transforms.Grayscale(), # not sure why the current input is not grayscale, do grayscale conversion\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    # should not do random transformation in val or test set\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((28,28)),\n",
    "        transforms.ToTensor(),\n",
    "    ]),\n",
    "}\n",
    "data_transforms = selected_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create dataset and dataloader\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 2. define and load data\n",
    "#\n",
    "# TODO: Suggest not using data loader to load the images. Because it is slow. Every epochs, you load the \n",
    "# image data again to the RAM and then from RAM to GPU RAM. That takes a lot of time. Since the images are \n",
    "# <1GB, you can preload the images into RAM first, that will make you training way faster. \n",
    "\n",
    "# Note: you only has about 200MB of images, it's small. It should not take much time to load!\n",
    "# ========================================\n",
    "print(\"Create dataset and dataloader\")\n",
    "\n",
    "# hyperparameter\n",
    "batch_size = 50 # larger batch size is better so you can load more data into gpu and train faster\n",
    "\n",
    "# data location\n",
    "book_data_dir = \"/home/kcho/clab_data/\"\n",
    "set_types = ['train', 'val', 'test']\n",
    "\n",
    "# test books are currently arbitrarily set\n",
    "test_books = set([\"Albin\", \"Dryden\"])\n",
    "\n",
    "# Get the list of all books in the data set\n",
    "books_in_data = set([b for b in os.listdir(book_data_dir)\n",
    "                 if os.path.isdir(os.path.join(book_data_dir, b))])\n",
    "\n",
    "# Create a dict of datasets for each book\n",
    "book_data_sets = {b : {t : datasets.ImageFolder(os.path.join(book_data_dir, b), \n",
    "                                                transform = data_transforms[t])#, transform=test_transform)\n",
    "                      for t in set_types}\n",
    "                 for b in books_in_data}\n",
    "\n",
    "# create a dict of dataloaders, book_data_loaders['Albin']['train']\n",
    "book_data_loaders = {b : {t : torch.utils.data.DataLoader(book_data_sets[b][t],\n",
    "                                                          batch_size=batch_size,\n",
    "                                                          shuffle=True, # make sure you shuffle the data\n",
    "                                                          num_workers=4)\n",
    "                          for t in set_types}\n",
    "                     for b in books_in_data}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 28, 28)\n",
      "(28, 28)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ========================================\n",
    "# Check your data to make sure it is correct\n",
    "# ========================================\n",
    "\n",
    "for data in book_data_loaders['Defoe']['train']:\n",
    "    # get the inputs\n",
    "    inputs, labels = data\n",
    "\n",
    "    # plot to verify the input is correct\n",
    "    input1 = inputs[0].numpy()\n",
    "    print(input1.shape)\n",
    "    input1 = np.swapaxes(input1,0,2).squeeze()\n",
    "    print(input1.shape)\n",
    "    plt.imshow(input1,cmap='gray')\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 3. define model structure\n",
    "#\n",
    "# TODO: Stick with MNIST for now because it is smaller network and faster to train. Note that you lose\n",
    "# a lot of inofmration when yo udownsample the image to 28x28. THat's okay, you optimize other parts first, e.g.,\n",
    "# data loading before you optimize your model.\n",
    "# ========================================\n",
    "from lib.playground.utee import selector\n",
    "\n",
    "def create_model_architecture(model_type='mnist'):\n",
    "    \"\"\"\n",
    "    params model_type: the type of model, for now, support mnist and resnet18    \n",
    "    \"\"\"\n",
    "    if model_type == 'mnist':\n",
    "        print('using pretrained mnist model')\n",
    "        model_annotation, ds_fetcher, is_imagenet = selector.select('mnist')\n",
    "        # remove last layer\n",
    "        removed = list(model_annotation.model.children())[:-1]\n",
    "        model_annotation.model=torch.nn.Sequential(*removed)\n",
    "        # add the new fc layer\n",
    "        model_annotation.model.fc = torch.nn.Linear(256,2).cuda()\n",
    "\n",
    "    elif model_type == 'resnet18':    \n",
    "        print(\"Transferring resnet18 and retraining with annotations dataset.\")    \n",
    "        model_annotation = models.resnet18(pretrained=True)\n",
    "        num_params = sum(1 for i in model_annotation.parameters())\n",
    "\n",
    "        # There are 10 layers (model_ft.children()) in resnet18\n",
    "        # Freezing the first half of resnet18, freezing all params for layers 1-5\n",
    "        max_layer = 5\n",
    "        curr_layer = 1\n",
    "        last_layer = None\n",
    "        for child in model_annotation.children():\n",
    "            if curr_layer <= max_layer:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "                last_layer = child\n",
    "                curr_layer = curr_layer + 1\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        # Replace the final fully connected layer to perform binary classification\n",
    "        num_ftrs = model_annotation.fc.in_features\n",
    "        model_annotation.fc = nn.Linear(num_ftrs, 2)\n",
    "        \n",
    "\n",
    "    # return\n",
    "    if use_gpu:\n",
    "        return model_annotation.cuda()\n",
    "    else:\n",
    "        return model_annotation.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Step 4. define the training process.\n",
    "#\n",
    "# TODO: The basic process of train and validation is defined. Please implement the overall average validation \n",
    "# confusion matrix, meaning for each validation (after running all epochs), get the confusion matrix for that book,\n",
    "# repeat this for the rest of 10 books. THen get the overall performance. Also, implement the early-stop as you\n",
    "# originally has in your code. :) I removed them here for clarity. You can add them back. \n",
    "# ========================================\n",
    "\n",
    "def train(model, criterion, optimizer, data_loaders, num_epochs=25, early_stopping = None):\n",
    "\n",
    "    # stop the training, validation, and test loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # looping parameters\n",
    "        running_loss = 0.0\n",
    "        confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "        \n",
    "        # loop through train and val phase in each epoch\n",
    "        for phase in ['train', 'val']:\n",
    "            # check train or val\n",
    "            if phase == 'train':                \n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            # Iterate over each book\n",
    "            running_loss = 0.0\n",
    "            confusion_matrix = tnt.meter.ConfusionMeter(2)\n",
    "            for book in tqdm_notebook(data_loaders[phase]):\n",
    "                for data in data_loaders[phase][book]:\n",
    "                    # get the inputs;  wrap them in Variable and make them into gpu or not\n",
    "                    inputs, labels = data # input size: [5, 1, 28, 28] ; keep the dummy color channel:1\n",
    "                    if use_gpu:\n",
    "                        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "                    else:\n",
    "                        inputs, labels = Variable(inputs), Variable(labels)\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs.data, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    # back\n",
    "                    if phase == 'train': \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # Add to confusion matrix\n",
    "                    confusion_matrix.add(outputs.data, labels.data)\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.data[0] * inputs.size(0) \n",
    "            \n",
    "            # report evaluation\n",
    "            print('Phase:%s' %phase)\n",
    "            print('Confusion matrix:\\n', confusion_matrix.conf)\n",
    "            print('validation loss', running_loss)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using pretrained mnist model\n",
      "Building and initializing mnist parameters\n",
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e023d904af1420ebf7dade11284c82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:58: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase:train\n",
      "Confusion matrix:\n",
      " [[ 116  502]\n",
      " [ 250 1776]]\n",
      "validation loss tensor(1520.6495, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f3db0921edf4772972ee71528b16acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase:val\n",
      "Confusion matrix:\n",
      " [[  0 143]\n",
      " [  0  31]]\n",
      "validation loss tensor(354.2864, device='cuda:0')\n",
      "Epoch 1/49\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb7c8e3311345d3aab26374dcd8d49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase:train\n",
      "Confusion matrix:\n",
      " [[   1  617]\n",
      " [   7 2019]]\n",
      "validation loss tensor(1591.5961, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72affa9fa7cf46389c0936623dc93811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase:val\n",
      "Confusion matrix:\n",
      " [[  0 143]\n",
      " [  0  31]]\n",
      "validation loss tensor(228.1974, device='cuda:0')\n",
      "Epoch 2/49\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2971e0f51aca40f6951a7f1ceebf1889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase:train\n",
      "Confusion matrix:\n",
      " [[   0  618]\n",
      " [   0 2026]]\n",
      "validation loss tensor(1601.9260, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "534bbc125c204e9e82fe937a0e35dfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase:val\n",
      "Confusion matrix:\n",
      " [[  0 143]\n",
      " [  0  31]]\n",
      "validation loss tensor(326.5123, device='cuda:0')\n",
      "Epoch 3/49\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5333175bec4ac391cc684044369a0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=9), HTML(value=u'')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-1173:\n",
      "Process Process-1170:\n",
      "Process Process-1172:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-1171:\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "Traceback (most recent call last):\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    self.run()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 267, in _bootstrap\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self.run()\n",
      "    self.run()\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/datasets/folder.py\", line 103, in __getitem__\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/usr/lib/python2.7/multiprocessing/process.py\", line 114, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "    sample = self.transform(sample)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/datasets/folder.py\", line 103, in __getitem__\n",
      "    img = t(img)\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "    r = index_queue.get()\n",
      "    sample = self.transform(sample)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n",
      "    img = t(img)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py\", line 822, in __call__\n",
      "    return F.rotate(img, angle, self.resample, self.expand, self.center)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/transforms.py\", line 175, in __call__\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-173-978c14aa54b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m                                                 \u001b[0mcross_val_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                                                 \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_training_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                                                 early_stopping = earlyStoppingCriteria)\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-172-472a031d7a99>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, data_loaders, num_epochs, early_stopping)\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0mconfusion_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfusionMeter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbook\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0;31m# get the inputs;  wrap them in Variable and make them into gpu or not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;31m# input size: [5, 1, 28, 28] ; keep the dummy color channel:1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_get_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader timed out after {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/multiprocessing/queues.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m()\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mracquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m                 \u001b[0mrrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.pyc\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    return recv()\n",
      "  File \"/usr/lib/python2.7/multiprocessing/queues.py\", line 376, in get\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/functional.py\", line 595, in rotate\n",
      "KeyboardInterrupt\n",
      "    buf = self.recv_bytes()\n",
      "    return img.rotate(angle, resample, expand, center)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    return F.resize(img, self.size, self.interpolation)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torchvision/transforms/functional.py\", line 206, in resize\n",
      "    return img.resize(size[::-1], interpolation)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/PIL/Image.py\", line 1749, in resize\n",
      "    return recv()\n",
      "    return self._new(self.im.resize(size, resample, box))\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/torch/multiprocessing/queue.py\", line 21, in recv\n",
      "    buf = self.recv_bytes()\n",
      "KeyboardInterrupt\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/PIL/Image.py\", line 1854, in rotate\n",
      "    return self.transform((w, h), AFFINE, matrix, resample)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/PIL/Image.py\", line 2137, in transform\n",
      "    resample, fillcolor is None)\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/PIL/Image.py\", line 2188, in __transformer\n",
      "    self.im.transform2(box, image.im, method, data, resample, fill)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# Step 5. execute the train process\n",
    "# ========================================\n",
    "\n",
    "# get the model\n",
    "model = create_model_architecture()\n",
    "\n",
    "# train parameters\n",
    "num_training_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "earlyStoppingCriteria = EarlyStopping(min_delta = 1e-4, patience=5)\n",
    "\n",
    "# dataloader parameters\n",
    "cross_val_loaders = {}\n",
    "\n",
    "# leave-one-book-out cross validation\n",
    "for val_book in books_in_data:\n",
    "    \n",
    "    # define the train and validation loaders\n",
    "    train_books = books_in_data - set([val_book])    \n",
    "    cross_val_loaders[\"train\"] = {b : book_data_loaders[b][\"train\"] for b in train_books}\n",
    "    cross_val_loaders[\"val\"] = {b : book_data_loaders[b][\"val\"] for b in [val_book]}\n",
    "\n",
    "    # train\n",
    "    trained_model_weights, epoch_scores = train(model,\n",
    "                                                criterion,\n",
    "                                                optimizer,\n",
    "                                                cross_val_loaders,\n",
    "                                                num_epochs=num_training_epochs,\n",
    "                                                early_stopping = earlyStoppingCriteria)\n",
    "\n",
    "print(\"training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
